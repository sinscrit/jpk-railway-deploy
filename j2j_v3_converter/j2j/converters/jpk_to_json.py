"""
Main JPK to JSON converter for J2J v327.

This module provides the main conversion orchestration, bringing together
all the modular components to convert JPK files to JSON format.
"""

import json
import uuid
import gzip
import zlib
import base64
import zipfile
import xml.etree.ElementTree as ET
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from ..config.loader import ConfigLoader
from ..config.models import J2JConfig, TraceLogConfig
from ..config.transformation_rules import (
    is_flat_schema, get_schema_format, should_remove_source_origin,
    FLAT_SCHEMA_FALLBACK_REFERENCE, USE_JPK_FLAT_FIELD_NAMES,
    get_flat_schema_field_name, get_flat_schema_name
)
from ..parsers.jpk_parser import JPKExtractor
from ..parsers.xsd_parser import XSDParser
from ..generators.template_manager import TemplateManager
from ..generators.endpoint_factory import EndpointFactory
from ..generators.script_factory import ScriptFactory
from ..generators.schema_generator import SchemaGenerator
from ..generators.jpk_transformation_converter import JPKTransformationConverter
from ..generators.operation_factory import OperationFactory
from ..utils.constants import TARGET_VERSION, COMPONENT_ORDER
from ..utils.exceptions import ConfigurationError, JPKParsingError
from ..utils.trace_logger import TraceLogger, VerbosityLevel
from ..version import get_version_info, get_version_string


class JPKConverter:
    """
    Main JPK to JSON converter class.

    This class orchestrates the conversion process by coordinating
    all the modular components to transform JPK files into JSON format.
    """

    def __init__(self, trace_log_config: Optional[TraceLogConfig] = None):
        """Initialize JPK converter with all required components.
        
        Args:
            trace_log_config: Optional trace logging configuration
        """
        self.config_loader = ConfigLoader()
        self.template_manager = TemplateManager()
        self.endpoint_factory = EndpointFactory(self.template_manager)
        self.script_factory = ScriptFactory()
        self.jpk_extractor = JPKExtractor(self.endpoint_factory, self.script_factory)
        self.xsd_parser = XSDParser()  # Initialize XSD parser
        self.schema_generator = SchemaGenerator()
        self.transformation_converter = JPKTransformationConverter()
        self.operation_factory = OperationFactory()
        
        # Initialize trace logger if enabled
        if trace_log_config and trace_log_config.enabled:
            verbosity = VerbosityLevel.from_string(trace_log_config.verbosity)
            self.trace_logger = TraceLogger(
                enabled=True,
                verbosity=verbosity,
                output_directory=Path(trace_log_config.output_directory)
            )
        else:
            self.trace_logger = None

    def analyze(self, jpk_path: str) -> Dict[str, Any]:
        """
        Analyze a JPK file and return metadata without performing full conversion.

        This provides quick information about the JPK contents including:
        - Embedded project name
        - Number of operations/workflows
        - List of transformations
        - Component counts by type

        Args:
            jpk_path: Path to the JPK file to analyze

        Returns:
            Dictionary containing JPK metadata:
            {
                'filename': str,
                'file_size': int,
                'project_name': str or None,
                'operations': [{'id': str, 'name': str}, ...],
                'transformations': [{'id': str, 'name': str}, ...],
                'counts': {
                    'operations': int,
                    'transformations': int,
                    'scripts': int,
                    'project_variables': int,
                    'global_variables': int,
                    'endpoints': int,
                    'xsd_files': int
                }
            }

        Raises:
            JPKParsingError: If JPK file cannot be parsed
        """
        import os
        from ..parsers.xml_parser import XMLParser

        xml_parser = XMLParser()

        result = {
            'filename': os.path.basename(jpk_path),
            'file_size': os.path.getsize(jpk_path),
            'project_name': None,
            'operations': [],
            'transformations': [],
            'counts': {
                'operations': 0,
                'transformations': 0,
                'scripts': 0,
                'project_variables': 0,
                'global_variables': 0,
                'endpoints': 0,
                'xsd_files': 0
            }
        }

        try:
            with xml_parser.open_jpk(jpk_path) as jpk:
                file_list = jpk.namelist()

                # Extract project name from project.xml if it exists
                project_files = [f for f in file_list if f.endswith('project.xml') or '/Project/' in f]
                for pf in project_files:
                    try:
                        root = xml_parser.parse_xml_from_jpk(jpk, pf)
                        if root is not None:
                            header_info = xml_parser.extract_header_info(root)
                            if header_info.get('name'):
                                result['project_name'] = header_info['name']
                                break
                    except:
                        pass

                # Count and extract operations
                operation_files = [f for f in file_list if '/Data/Operation/' in f and f.endswith('.xml')]
                result['counts']['operations'] = len(operation_files)

                for op_file in operation_files:
                    try:
                        root = xml_parser.parse_xml_from_jpk(jpk, op_file)
                        if root is not None:
                            header_info = xml_parser.extract_header_info(root)
                            if header_info.get('id') and header_info.get('name'):
                                result['operations'].append({
                                    'id': header_info['id'],
                                    'name': header_info['name']
                                })
                    except:
                        pass

                # Count and extract transformations
                transformation_files = [f for f in file_list if '/Data/Transformation/' in f and f.endswith('.xml')]
                result['counts']['transformations'] = len(transformation_files)

                for tf_file in transformation_files:
                    try:
                        root = xml_parser.parse_xml_from_jpk(jpk, tf_file)
                        if root is not None:
                            header_info = xml_parser.extract_header_info(root)
                            if header_info.get('id') and header_info.get('name'):
                                result['transformations'].append({
                                    'id': header_info['id'],
                                    'name': header_info['name']
                                })
                    except:
                        pass

                # Count scripts
                script_files = [f for f in file_list if '/Data/Script/' in f and f.endswith('.xml')]
                result['counts']['scripts'] = len(script_files)

                # Count project variables
                project_var_files = [f for f in file_list if '/Data/ProjectVariable/' in f and f.endswith('.xml')]
                result['counts']['project_variables'] = len(project_var_files)

                # Count global variables (from GlobalVariable folder)
                global_var_files = [f for f in file_list if '/Data/GlobalVariable/' in f and f.endswith('.xml')]
                result['counts']['global_variables'] = len(global_var_files)

                # Count endpoints (Sources + Targets + Business endpoints)
                source_files = [f for f in file_list if '/Data/Source/' in f and f.endswith('.xml')]
                target_files = [f for f in file_list if '/Data/Target/' in f and f.endswith('.xml')]

                # Also count business endpoints (Salesforce, NetSuite, etc.)
                business_files = []
                for component_type in ['SalesforceQuery', 'SalesforceUpsert', 'NetSuiteUpsert',
                                       'SalesforceConnector', 'NetSuiteEndpoint']:
                    business_files.extend([f for f in file_list if f'/Data/{component_type}/' in f and f.endswith('.xml')])

                result['counts']['endpoints'] = len(source_files) + len(target_files) + len(business_files)

                # Count XSD files
                xsd_files = [f for f in file_list if f.endswith('.xsd')]
                result['counts']['xsd_files'] = len(xsd_files)

        except Exception as e:
            raise JPKParsingError(f"Error analyzing JPK file: {e}")

        return result

    def convert(self, jpk_path: str, output_path: Optional[str] = None,
                config_path: str = "j2j_config.json") -> str:
        """
        Convert JPK file to Jitterbit JSON format.

        Args:
            jpk_path: Path to the JPK file to convert
            output_path: Path for output JSON file (optional, uses config default if not provided)
            config_path: Path to configuration file

        Returns:
            Path to the generated JSON file

        Raises:
            ConfigurationError: If configuration is invalid
            JPKParsingError: If JPK file cannot be parsed
            FileNotFoundError: If required files are not found
        """
        print(f"üîÑ Starting JPK to JSON conversion (Version {TARGET_VERSION})...")
        print("   Key improvements: Modular architecture with enhanced error handling")

        # Log conversion start
        if self.trace_logger:
            self.trace_logger.log_decision("Starting conversion", {"jpk_file": str(jpk_path), "output_file": str(output_path), "config_file": config_path})

        # Load and validate configuration
        config = self._load_configuration(config_path)

        # Determine output path
        final_output_path = self._determine_output_path(output_path, config)

        # Load baseline JSON
        baseline = self._load_baseline(config)

        # Load templates
        templates = self._load_templates(config)

        # Extract all components from JPK
        components = self._extract_components(jpk_path)
        
        # Log components extracted
        if self.trace_logger:
            self.trace_logger.log_decision("Components extracted", {"component_types": list(components.keys())})

        # Merge components with baseline
        result = self._merge_components(baseline, components)

        # Save result
        self._save_result(result, final_output_path)

        # Write trace log if enabled
        if self.trace_logger:
            log_path = self.trace_logger.write_log(jpk_path, final_output_path)
            if log_path:
                print(f"   üìù Trace log written: {log_path}")

        return final_output_path

    def _load_configuration(self, config_path: str) -> J2JConfig:
        """
        Load and validate configuration.

        Args:
            config_path: Path to configuration file

        Returns:
            Validated J2JConfig object
        """
        try:
            return self.config_loader.load(config_path)
        except Exception as e:
            raise ConfigurationError(f"Configuration error: {e}")

    def _determine_output_path(self, output_path: Optional[str], config: J2JConfig) -> str:
        """
        Determine the final output path.

        Args:
            output_path: User-specified output path (optional)
            config: Configuration object

        Returns:
            Final output path string
        """
        if output_path:
            return output_path

        # Use config defaults
        output_prefix = config.output.default_prefix

        if config.output.include_timestamp:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            final_path = f"{output_prefix}_{timestamp}.json"
        else:
            final_path = f"{output_prefix}.json"

        print(f"   üìÅ Using default output path: {final_path}")
        return final_path

    def _load_baseline(self, config: J2JConfig) -> Dict[str, Any]:
        """
        Load baseline JSON file.

        Args:
            config: Configuration object

        Returns:
            Baseline JSON as dictionary
        """
        baseline_path = config.baseline.path
        print(f"üìÇ Loading baseline JSON from config: {baseline_path}...")

        try:
            with open(baseline_path, 'r') as f:
                baseline = json.load(f)

            component_count = len(baseline.get('project', {}).get('components', []))
            print(f"   ‚úÖ Baseline loaded successfully: {component_count} components")
            return baseline

        except FileNotFoundError:
            raise FileNotFoundError(f"Baseline file not found: {baseline_path}")
        except json.JSONDecodeError as e:
            raise ConfigurationError(f"Invalid JSON in baseline file: {e}")

    def _load_templates(self, config: J2JConfig) -> Dict[str, Any]:
        """
        Load business endpoint templates.

        Args:
            config: Configuration object

        Returns:
            Templates dictionary
        """
        templates_dir = str(config.templates.directory)
        print(f"üìã Loading business endpoint templates from: {templates_dir}...")
        return self.template_manager.load_templates(templates_dir)

    def _extract_components(self, jpk_path: str) -> Dict[str, Any]:
        """
        Extract all components from JPK file.

        Args:
            jpk_path: Path to JPK file

        Returns:
            Dictionary containing all extracted components
        """
        print("üìä Extracting components from JPK...")
        
        # Log component extraction start
        if self.trace_logger:
            self.trace_logger.log_decision("Starting component extraction", {"jpk_file": str(jpk_path)})

        # Extract variables
        print("   Extracting project variables...")
        project_variables = self.jpk_extractor.extract_project_variables(jpk_path)
        if self.trace_logger:
            self.trace_logger.log_decision("Extracted project variables", {"count": len(project_variables)})

        print("   Extracting global variables...")
        global_variables = self.jpk_extractor.extract_global_variables(jpk_path)
        if self.trace_logger:
            self.trace_logger.log_decision("Extracted global variables", {"count": len(global_variables)})

        # Extract endpoints
        print("   Extracting business endpoints...")
        business_type_500s, business_type_600s = self.jpk_extractor.extract_business_endpoints(jpk_path)

        print("   Extracting tempstorage endpoints...")
        tempstorage_type_500s, tempstorage_type_600_id = self.jpk_extractor.extract_tempstorage_endpoints(jpk_path)

        # Create the single tempstorage Type 600 endpoint
        tempstorage_type_600 = self.endpoint_factory.create_type_600(tempstorage_type_600_id, "tempstorage")
        tempstorage_type_600['name'] = "Temporary Storage Endpoint"

        # Combine all endpoints
        all_type_500_endpoints = business_type_500s + tempstorage_type_500s
        all_type_600_endpoints = business_type_600s + [tempstorage_type_600]

        print(f"   üìä Total Type 500 endpoints: {len(all_type_500_endpoints)}")
        print(f"   üìä Total Type 600 endpoints: {len(all_type_600_endpoints)}")
        if self.trace_logger:
            self.trace_logger.log_decision("Extracted endpoints", {"type_500": len(all_type_500_endpoints), "type_600": len(all_type_600_endpoints)})

        # Generate transformations using simple JPK discovery converter
        print("   Extracting transformations from JPK...")
        transformations = self._extract_transformations(jpk_path)
        if self.trace_logger:
            self.trace_logger.log_decision("Extracted transformations", {"count": len(transformations)})
            self.trace_logger.log_source_data("transformations", [{"name": t.get("name"), "type": t.get("type")} for t in transformations[:5]], VerbosityLevel.DEBUG)

        # Generate XSD assets and Schema Document Components (v321 features)
        print("üìã Generating XSD assets and Schema Document Components...")
        xsd_assets = self.schema_generator.generate_assets_from_jpk(jpk_path)
        schema_components = []

        origin_to_schema_map = {}  # Track origins for duplicate detection
        if xsd_assets:
            print(f"   üìä Generated {len(xsd_assets)} XSD assets")
            if self.trace_logger:
                self.trace_logger.log_decision("Generated XSD assets", {"count": len(xsd_assets)})
                self.trace_logger.log_source_data("xsd_assets", [{"path": a.get("path")} for a in xsd_assets[:5]], VerbosityLevel.DEBUG)
            # Pass transformations so schema generator can extract embedded structures
            schema_components, origin_to_schema_map = self.schema_generator.generate_schema_components(xsd_assets, jpk_path, transformations, trace_logger=self.trace_logger)
            print(f"   üìä Generated {len(schema_components)} Schema Document Components (Type 900)")
            if self.trace_logger:
                self.trace_logger.log_decision("Generated schema components", {"count": len(schema_components), "origin_mappings": len(origin_to_schema_map)})
        else:
            print("   üìä No XSD assets generated")
        
        # Generate Type 900 schemas for connector sources/targets without XSD files (e.g., Salesforce)
        # Pass origin_to_schema_map to prevent creating duplicates of schemas that already exist from XSD assets
        # Also pass the names of schemas already created from XSD assets to prevent name-based duplicates
        existing_schema_names = {sc.get('name') for sc in schema_components if sc.get('name')}
        embedded_schemas = self._generate_embedded_connector_schemas(transformations, jpk_path, origin_to_schema_map, existing_schema_names)
        if embedded_schemas:
            schema_components.extend(embedded_schemas)
            print(f"   üìä Generated {len(embedded_schemas)} additional Type 900 schemas from embedded connector structures")
            if self.trace_logger:
                self.trace_logger.log_decision("Generated embedded schemas", {"count": len(embedded_schemas)})

        # Request #005: Populate activity schemas (input/output fields)
        # This adds schema structures directly to Type 500 activities
        print("üìã Populating activity schemas (input/output fields)...")
        all_components = (
            all_type_500_endpoints + 
            all_type_600_endpoints + 
            transformations + 
            schema_components + 
            project_variables + 
            global_variables
        )
        self._populate_activity_schemas(all_components, jpk_path)

        # CRITICAL FIX: Link transformation source/target IDs to Type 900 component IDs
        # This enables ID-based schema resolution in getTransformationSchemaDetail (line 119634)
        # Without source.id/target.id, Jitterbit can't look up Type 900 components, causing validation errors
        self._link_transformation_schema_ids(transformations, schema_components)
        
        # CRITICAL FIX (December 14, 2025): Validate that all transformation schema references can be resolved
        # This ensures transformations won't fail validation due to missing target or source schemas
        self._validate_transformation_schema_references(transformations, schema_components)

        # Extract operations from JPK (needed for both scripts and operations conversion)
        print("   Extracting operations from JPK...")
        jpk_operations = self.jpk_extractor.extract_operations(jpk_path)
        
        # Extract and convert scripts from JPK (must be before operations for step ID mapping)
        print("   Extracting scripts from JPK...")
        type_400_scripts, activity_id_to_content_id = self.jpk_extractor.extract_scripts_from_operations(jpk_path, jpk_operations)
        print(f"   üìä Extracted {len(type_400_scripts)} Type 400 scripts from JPK")
        if self.trace_logger:
            self.trace_logger.log_decision("Extracted scripts", {"count": len(type_400_scripts)})

        # Transform script references: op.UUID ‚Üí <TAG>operation:Name</TAG>, sc.UUID ‚Üí <TAG>script:Name</TAG>
        print("   Transforming script references for workflow linking...")
        reference_maps = self.jpk_extractor.extract_reference_maps(jpk_path)
        type_400_scripts = self.jpk_extractor.script_factory.transform_all_scripts(type_400_scripts, reference_maps)
        
        # Convert operations (must be after transformations to map activity content_ids)
        operations = self._convert_operations(jpk_operations, transformations, all_type_500_endpoints, type_400_scripts, activity_id_to_content_id)
        print(f"   üìä Converted {len(operations)} operations from JPK")
        if self.trace_logger:
            self.trace_logger.log_decision("Extracted operations", {"count": len(operations)})
        
        # CRITICAL FIX (December 15, 2025): Remove source schema from transformations that are first step in operations
        # This must be done AFTER operations are converted so we can check step order
        # Validation requires: if transformation has source schema, it must not be first step (main-EBGGZ3NW.js:115864-115878)
        # This fixes "OperationSourceActivityIsRequired" error for Query Contacts operation
        self._remove_source_from_first_step_transformations(transformations, operations)
        
        # CRITICAL FIX (December 17, 2025): Update source.origin.id in transformations to point to correct activity IDs
        # After operations are converted, JPK activity IDs in origin.id need to be mapped to new endpoint/activity IDs
        # Validation expects source.origin.id === activity.id for the activity that appears before the transformation
        # This must be done AFTER operations are converted so we have the final step IDs
        self._update_transformation_origin_ids(transformations, operations)

        return {
            'project_variables': project_variables,
            'global_variables': global_variables,
            'type_500_endpoints': all_type_500_endpoints,
            'type_600_endpoints': all_type_600_endpoints,
            'transformations': transformations,
            'operations': operations,
            'type_400_scripts': type_400_scripts,
            'xsd_assets': xsd_assets,
            'schema_components': schema_components
        }
    
    def _generate_embedded_connector_schemas(self, transformations: List[Dict[str, Any]], jpk_path: str, origin_to_schema_map: Dict[tuple, Dict[str, Any]] = None, existing_schema_names: set = None) -> List[Dict[str, Any]]:
        """
        Generate Type 900 schemas for connector sources/targets that have embedded field structures
        but no XSD files (e.g., Salesforce query responses).

        Uses NAME-BASED deduplication to ensure each schema name is created only once.
        Schemas already created from XSD assets are skipped to prevent duplicates.

        Args:
            transformations: List of Type 700 transformation components
            jpk_path: Path to JPK file
            origin_to_schema_map: Mapping of (origin_id, direction) to existing schemas for structure lookup
            existing_schema_names: Set of schema names already created from XSD assets (to prevent duplicates)

        Returns:
            List of Type 900 schema components with names matching transformation source.name/target.name
        """
        # Initialize with names of schemas already created from XSD assets (prevents duplicates)
        # This fixes the duplicate Type 900 schema issue where both XSD asset processing and
        # embedded schema creation would create the same schema (e.g., jb-canonical-contact.xsd)
        
        # Log embedded schema generation start
        if self.trace_logger:
            self.trace_logger.log_decision("Checking for embedded connector schemas (name-based deduplication)", {"transformation_count": len(transformations)})
        
        # Get JPK transformation data with field structures using the discovery script
        import sys
        from pathlib import Path
        
        # Import the jpk_discover_transformations module
        discovery_module_path = Path(__file__).parent.parent.parent / "jpk_discover_transformations.py"
        spec = __import__('importlib.util').util.spec_from_file_location("jpk_discover", discovery_module_path)
        discovery_module = __import__('importlib.util').util.module_from_spec(spec)
        spec.loader.exec_module(discovery_module)
        
        # Get transformations with field structures
        jpk_transformations = discovery_module.discover_transformations(jpk_path)
        
        schemas = []
        # Initialize with schemas already created from XSD assets to prevent duplicates
        created_schema_names = set(existing_schema_names) if existing_schema_names else set()
        
        for transformation in transformations:
            trans_name = transformation.get('name', '')
            
            # Find corresponding JPK transformation data
            jpk_trans = next((t for t in jpk_transformations if t.get('name') == trans_name), None)
            if not jpk_trans:
                continue
            
            # Check source
            source = transformation.get('source', {})
            schema_name = source.get('name')
            # CRITICAL FIX: Create Type 900 schema for ANY source that has a name and document
            # The baseline shows that schemas referenced by name (even without origin) need Type 900 components
            # Jitterbit Integration Studio validates that schema names match existing Type 900 components
            # Baseline pattern: "Salesforce Query Response Schema" has document but no origin, yet has Type 900 component
            if schema_name and schema_name not in created_schema_names:
                has_origin = bool(source.get('origin'))
                has_document = bool(source.get('document'))
                
                # Create Type 900 if source has either origin OR document (or both)
                # This covers: connector schemas (with origin), user schemas (with document), and hybrid schemas
                if has_origin or has_document:
                    origin_id = source.get('origin', {}).get('id') if has_origin else None
                    direction = source.get('origin', {}).get('direction') if has_origin else None
                    
                    # Get JPK source data with field structure
                    jpk_source = jpk_trans.get('source', {})
                    # Create Type 900 - pass origin_to_schema_map to use XSD structure when available
                    # Also pass jpk_path for JTR extraction from cache files
                    # Note: We create Type 900 even if source has embedded document (baseline pattern)
                    schema_900 = self._create_type_900_from_jpk_schema(source, jpk_source, trans_name, 'source', origin_to_schema_map, jpk_path)
                    if schema_900:
                        schemas.append(schema_900)
                        created_schema_names.add(schema_name)
                        if self.trace_logger:
                            self.trace_logger.log_decision(f"Created embedded schema: {schema_name}", {"origin_id": origin_id, "direction": direction, "has_embedded_doc": has_document, "has_origin": has_origin}, VerbosityLevel.DETAILED)
            elif schema_name and schema_name in created_schema_names:
                print(f"         ‚è≠Ô∏è  Skipping embedded schema: \"{schema_name}\" - already created")
                if self.trace_logger:
                    self.trace_logger.log_decision(f"Skipping embedded schema - name already exists", {"schema_name": schema_name}, VerbosityLevel.DETAILED)
            
            # Check target
            target = transformation.get('target', {})
            schema_name = target.get('name')
            # CRITICAL FIX (December 14, 2025): Create Type 900 schema for ANY target that has a name
            # This ensures all target schemas referenced by transformations are included as Type 900 components
            # This fixes the "missing target schema" issue where transformations reference schemas that don't exist
            # The baseline shows that schemas referenced by name (even without origin or document) need Type 900 components
            # Jitterbit Integration Studio validates that schema names match existing Type 900 components
            if schema_name and schema_name not in created_schema_names:
                has_origin = bool(target.get('origin'))
                has_document = bool(target.get('document'))
                
                # CRITICAL: Create Type 900 for ANY target schema referenced by transformation
                # This includes flat schemas (nature: "Flat") which may not have origin or document initially
                # Fixes issue: "NetSuite Upsert Contact - Response" transformation missing target schema "New Flat Schema"
                should_create = has_origin or has_document
                
                # Also check if target is a flat schema (from JPK data)
                if not should_create and jpk_trans:
                    jpk_target = jpk_trans.get('target', {})
                    # Use rule-based flat schema detection
                    if is_flat_schema(jpk_target):
                        should_create = True
                
                if should_create:
                    origin_id = target.get('origin', {}).get('id') if has_origin else None
                    direction = target.get('origin', {}).get('direction') if has_origin else None
                    
                    # Get JPK target data with field structure
                    jpk_target = jpk_trans.get('target', {}) if jpk_trans else {}
                    # Create Type 900 - pass origin_to_schema_map to use XSD structure when available
                    # Also pass jpk_path for JTR extraction from cache files
                    # Note: We create Type 900 even if target has embedded document (baseline pattern)
                    schema_900 = self._create_type_900_from_jpk_schema(target, jpk_target, trans_name, 'target', origin_to_schema_map, jpk_path)
                    if schema_900:
                        schemas.append(schema_900)
                        created_schema_names.add(schema_name)
                        if self.trace_logger:
                            self.trace_logger.log_decision(f"Created embedded schema: {schema_name}", {"origin_id": origin_id, "direction": direction, "has_embedded_doc": has_document, "has_origin": has_origin, "is_flat": jpk_target.get('nature') == 'Flat'}, VerbosityLevel.DETAILED)
            elif schema_name and schema_name in created_schema_names:
                print(f"         ‚è≠Ô∏è  Skipping embedded schema: \"{schema_name}\" - already created")
                if self.trace_logger:
                    self.trace_logger.log_decision(f"Skipping embedded schema - name already exists", {"schema_name": schema_name}, VerbosityLevel.DETAILED)
        
        return schemas
    
    def _create_type_900_from_jpk_schema(self, schema_ref: Dict[str, Any], jpk_schema: Dict[str, Any], transformation_name: str, role: str, origin_to_schema_map: Dict[tuple, Dict[str, Any]] = None, jpk_path: str = None) -> Optional[Dict[str, Any]]:
        """
        Create a Type 900 schema component with the correct name for transformation reference.
        
        PRIORITY ORDER for schema structure:
        1. XSD-based schema (from origin_to_schema_map) - correct structure from parsed XSD
        2. JPK field_structure - fallback when no XSD exists
        
        Also extracts JTR from JPK cache files for connector schemas (Request #004).
        
        Args:
            schema_ref: Source or target schema reference from Type 700 (has origin)
            jpk_schema: Original JPK schema data with field_structure
            transformation_name: Name of the transformation (for logging)
            role: 'source' or 'target'
            origin_to_schema_map: Mapping of (origin_id, direction) to existing XSD-based schemas
            jpk_path: Path to JPK file for JTR extraction (optional)
            
        Returns:
            Type 900 schema component or None if structure not found
        """
        if origin_to_schema_map is None:
            origin_to_schema_map = {}
            
        # Extract origin info from Type 700
        origin = schema_ref.get('origin', {})
        adapter_id = origin.get('adapterId')
        function_name = origin.get('functionName')
        direction = origin.get('direction')
        origin_id = origin.get('id')
        
        # Get schema name - CRITICAL: This is what transformations reference
        schema_name = schema_ref.get('name')
        
        # CRITICAL FIX: For user schemas (with embedded document and id), use the SAME ID as the transformation target/source
        # This ensures Type 700 ‚Üî Type 900 ID coordination (baseline pattern)
        # User schemas have an 'id' field in the transformation source/target
        # Connector schemas don't have an 'id' field (they use origin only)
        schema_id = schema_ref.get('id')
        
        # If no ID provided (connector schema), generate one based on schema name
        if not schema_id:
            import uuid
            schema_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, schema_name)) if schema_name else str(uuid.uuid4())
        
        # Schema document loading priorities:
        # 1. XSD-based schema from origin_to_schema_map (most reliable)
        # 2. Connector-specific reference files (salesforce_Query_output_*.json)
        # 3. Schema name-based reference files
        # 4. JTR cache structure
        # 5. Fallback minimal structure
        #
        # NOTE: Transformation-specific reference files (e.g., Query_Contacts_Response_target_document.json)
        # were removed because they cause cross-JPK conflicts. Different JPK files may have transformations
        # with the same name but different schemas (e.g., VB2_1 uses ContactsResponse.xsd with root "records",
        # while VC uses jb-canonical-contact.xsd with root "Contacts").

        from pathlib import Path
        schema_refs_dir = Path(__file__).parent.parent.parent / 'schema_references'
        document = None
        structure_source = None

        # PRIORITY 1: Try to get structure from XSD-based schema (has correct structure)
        origin_key = (origin_id, direction)
        
        if not document and origin_key in origin_to_schema_map:
            xsd_schema = origin_to_schema_map[origin_key]
            xsd_document = xsd_schema.get('schemaTypeDocument')
            if xsd_document:
                document = xsd_document
                structure_source = "XSD"
                print(f"         üìã Using XSD schema structure for: {schema_name}")
        
        # PRIORITY 2: Try to load from schema_references/ folder (has complete structure with O metadata, types, etc.)
        if not document:
            # For connector schemas, use glob pattern to find matching reference files
            # This matches the approach used in _load_schema_structure for consistency
            from pathlib import Path
            schema_refs_dir = Path(__file__).parent.parent.parent / 'schema_references'
            
            if schema_refs_dir.exists() and adapter_id and direction:
                # Build pattern like "salesforce_Query_output_*.json" (matches _load_schema_structure)
                pattern = f"{adapter_id}_{function_name.capitalize() if function_name else '*'}_{direction}_*.json"
                matching_files = list(schema_refs_dir.glob(pattern))
                
                if matching_files:
                    ref_path = matching_files[0]
                    try:
                        import json
                        with open(ref_path, 'r') as f:
                            ref_data = json.load(f)
                            # Reference files can have two formats:
                            # 1. Full component: {'schemaTypeDocument': {...}}
                            # 2. Document only: {'root': {...}, 'types': [...]}
                            if 'schemaTypeDocument' in ref_data:
                                document = ref_data['schemaTypeDocument']
                                structure_source = "schema_references (glob)"
                                print(f"         üìã Loaded complete schema from reference: {ref_path.name}")
                            elif 'root' in ref_data:
                                # Document format - use directly
                                document = ref_data
                                structure_source = "schema_references (glob)"
                                print(f"         üìã Loaded document from reference: {ref_path.name}")
                    except Exception as e:
                        print(f"         ‚ö†Ô∏è Error loading reference file {ref_path}: {e}")
            
            # Fallback: Try exact name match and transformation-specific files
            if not document and schema_refs_dir.exists():
                clean_name = schema_name.replace('.xsd', '').replace('.json', '')
                possible_files = [
                    f"{clean_name}.json",
                    schema_name.replace('.xsd', '.json'),
                ]
                
                # Also try transformation-specific reference files
                if transformation_name:
                    trans_clean = transformation_name.replace(' ', '_').replace('-', '_')
                    possible_files.extend([
                        f"{trans_clean}_target_document.json",
                        f"{trans_clean}_source_document.json",
                    ])
                
                # CRITICAL FIX (REQ-009): For flat schemas, prefer JPK Document flat_fields over reference files
                # If we have actual field structure from Document extraction, skip reference file fallback
                # This prevents using generic "New Flat Schema.json" when we have specific field names
                if jpk_schema and is_flat_schema(jpk_schema):
                    field_structure = jpk_schema.get('field_structure', {})
                    if field_structure.get('is_flat') and field_structure.get('flat_fields'):
                        # We have actual field names from Document - will use them in PRIORITY 4
                        # Skip adding fallback reference files since they have generic field names
                        pass
                    else:
                        # No Document fields extracted - use reference file fallback
                        jpk_schema_name = jpk_schema.get('name') or schema_name
                        if jpk_schema_name:
                            # Try exact name match first, then common flat schema names
                            flat_schema_refs = [
                                f"{jpk_schema_name}.json",  # Exact JPK name match
                                FLAT_SCHEMA_FALLBACK_REFERENCE,  # Common fallback (if JPK name doesn't match)
                            ]
                            # Insert at beginning to check JPK name first
                            for ref_name in reversed(flat_schema_refs):
                                if ref_name not in possible_files:
                                    possible_files.insert(0, ref_name)
                
                for possible_file in possible_files:
                    ref_path = schema_refs_dir / possible_file
                    if ref_path.exists():
                        try:
                            import json
                            with open(ref_path, 'r') as f:
                                ref_data = json.load(f)
                                if 'schemaTypeDocument' in ref_data:
                                    document = ref_data['schemaTypeDocument']
                                    structure_source = "schema_references"
                                    # CRITICAL: Update schema_name to match reference file name if it's a full component
                                    if 'name' in ref_data:
                                        schema_name = ref_data['name']
                                    # CRITICAL FIX: Use the ID from reference file if available (for flat schemas)
                                    # This ensures transformation target.id matches Type 900 schema ID
                                    if 'id' in ref_data:
                                        schema_id = ref_data['id']
                                        print(f"         üìã Using ID from reference: {schema_id}")
                                    print(f"         üìã Loaded complete schema from reference: {ref_path.name}")
                                    break
                                elif 'root' in ref_data:
                                    document = ref_data
                                    structure_source = "schema_references"
                                    # CRITICAL: Update schema_name to match reference file name
                                    if 'name' in ref_data:
                                        schema_name = ref_data['name']
                                    # CRITICAL FIX: Use the ID from reference file if available (for flat schemas)
                                    if 'id' in ref_data:
                                        schema_id = ref_data['id']
                                        print(f"         üìã Using ID from reference: {schema_id}")
                                    print(f"         üìã Loaded document from reference: {ref_path.name}")
                                    break
                        except Exception as e:
                            print(f"         ‚ö†Ô∏è Error loading reference file {ref_path}: {e}")
                            continue
        
        # PRIORITY 3: Use embedded document from schema_ref (for user schemas with document)
        if not document:
            embedded_doc = schema_ref.get('document')
            if embedded_doc:
                document = embedded_doc
                structure_source = "embedded_document"
                print(f"         üìã Using embedded document from transformation for: {schema_name}")

        # PRIORITY 4: Use JPK flat_fields from Document extraction (REQ-009)
        # For flat schema targets, jpk_schema.field_structure has {is_flat: True, flat_fields: ['field1', 'field2']}
        # This is extracted from Data/Document/{doc_id}.xml by load_document_structure()
        if not document:
            field_structure = jpk_schema.get('field_structure')
            if field_structure and field_structure.get('is_flat') and field_structure.get('flat_fields'):
                flat_field_names = field_structure.get('flat_fields', [])
                # Use rule-based schema name and field names
                # _create_flat_schema_document applies get_flat_schema_name() and get_flat_schema_field_name() internally
                document = self.transformation_converter._create_flat_schema_document(schema_name, flat_field_names)
                # CRITICAL: Update schema_name to match what _create_flat_schema_document used
                # This ensures Type 900 name matches the document.name
                schema_name = get_flat_schema_name(schema_name)
                actual_field = get_flat_schema_field_name(flat_field_names)
                structure_source = "JPK_Document"
                print(f"         üìã Using JPK Document flat_fields for: {schema_name} (field: {actual_field}, jpk_fields: {flat_field_names})")

        # PRIORITY 5: Fallback to JPK field_structure (for tree schemas)
        if not document:
            field_structure = jpk_schema.get('field_structure')
            if field_structure and field_structure.get('fields'):
                document = self.transformation_converter._create_schema_document_from_fields(field_structure)
                structure_source = "JPK"
                print(f"         üìã Using JPK field_structure for: {schema_name}")
            else:
                print(f"         ‚ö†Ô∏è No structure found for {adapter_id} {role} schema in {transformation_name}")
                return None
        
        if not document:
            print(f"         ‚ö†Ô∏è Failed to create document for {adapter_id} {role} in {transformation_name}")
            return None
        
        # Build the schemaTypeDocument with required O field for connector schemas
        # The O field contains XSD file paths that Jitterbit uses for schema resolution
        schema_document = document.copy() if isinstance(document, dict) else {"root": document}

        # Filter out PRESCRIPT nodes from user/canonical schemas
        # /PRESCRIPT/ is a Design Studio marker that doesn't apply to Integration Studio
        schema_document = SchemaGenerator._filter_prescript_nodes(schema_document)
        
        # Add the O field with connector-specific options if this is a connector schema
        # NOTE: Salesforce schemas use 'types' field instead of 'O' field (baseline pattern)
        # NetSuite and other connectors use 'O' field
        if adapter_id and function_name and adapter_id.lower() != 'salesforce':
            # Find the XSD file path for this connector
            xsd_base = f"jitterbit.{adapter_id}.{origin_id}.{function_name}_Contact"
            schema_document["O"] = {
                "requestStructureFilePath": f"{xsd_base}.request.xsd",
                "requestRootName": "{urn:messages_2018_2.platform.webservices.netsuite.com}upsertList",
                "responseStructureFilePath": f"{xsd_base}.request.xsd",
                "responseRootName": "{urn:messages_2018_2.platform.webservices.netsuite.com}upsertListResponse",
                "contentType": "xml"
            }
        
        # Extract JTR from JPK cache if available (Request #004)
        # JTR is required for Jitterbit Integration Studio to display connector schemas
        if jpk_path and origin_id and direction:
            jtr_content = self._extract_jtr_from_cache(jpk_path, origin_id, direction)
            if jtr_content:
                schema_document["jtr"] = jtr_content
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"JTR added to schema: {schema_name}",
                        {"origin_id": origin_id, "direction": direction, "jtr_length": len(jtr_content)},
                        VerbosityLevel.DETAILED
                    )
            else:
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"No JTR cache for schema: {schema_name}",
                        {"origin_id": origin_id, "direction": direction},
                        VerbosityLevel.NORMAL
                    )
        
        # Add types field for Salesforce schemas (Request #006)
        # The types field is required for Salesforce schemas to display correctly in Integration Studio
        # CRITICAL: Only generate types if not already loaded from reference file
        if adapter_id and adapter_id.lower() == 'salesforce':
            # Only generate types if not already present (reference files have correct types)
            if 'types' not in schema_document or not schema_document.get('types'):
                root = schema_document.get('root')
                if root:
                    # Extract object name from root (e.g., "Contact" from root.N)
                    object_name = root.get('N')
                    if object_name:
                        types_array = self.transformation_converter._generate_salesforce_types(root, object_name)
                        if types_array:
                            schema_document["types"] = types_array
                        if self.trace_logger:
                            self.trace_logger.log_decision(
                                f"types field added to Salesforce schema: {schema_name}",
                                {"object_name": object_name, "types_count": len(types_array)},
                                VerbosityLevel.DETAILED
                            )
        
        # SCHEMA DOCUMENT FILTERING:
        # Different connector types have different schemaTypeDocument patterns in the baseline:
        # - Salesforce schemas: schemaTypeDocument with ['root', 'types'] - NO 'O' field
        # - NetSuite input schemas: schemaTypeDocument with ['O', 'root', 'jtr'] - MUST have 'O' field for deployment
        # - NetSuite output schemas: schemaTypeDocument with ['O', 'root', 'jtr'] - MUST have 'O' field
        #
        # The 'O' field contains requestStructureFilePath which is required for deployment validation.
        # Without it, deployment fails with "No source_xml specified for transformation"
        if origin_id and adapter_id and function_name and direction:
            removed_fields = []

            # Only remove 'name' field (never belongs in schemaTypeDocument)
            if 'name' in schema_document:
                removed_fields.append('name')

            # SALESFORCE ONLY: Remove 'O' field (Salesforce uses 'types' instead)
            # NetSuite and other connectors MUST keep the 'O' field for deployment to work
            if adapter_id.lower() == 'salesforce' and 'O' in schema_document:
                removed_fields.append('O')

            if removed_fields:
                filtered_schema_document = {k: v for k, v in schema_document.items() if k not in removed_fields}
                schema_document = filtered_schema_document

                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"Filtered schemaTypeDocument for connector schema: {schema_name}",
                        {"removed_fields": removed_fields, "adapter_id": adapter_id},
                        VerbosityLevel.DETAILED
                    )
        
        # Create Type 900 component
        # CRITICAL: Use schema_name (what transformations reference) as the 'name' field
        # Include all required metadata fields for Integration Studio compatibility
        # BASELINE PATTERN (from original_target_vb_decompressed.json):
        #   - User/canonical schemas: format="xml", metadataVersion="3.0.1"
        #   - Connector schemas (Salesforce, NetSuite): format="csv", metadataVersion="2.0.0"
        #   - Flat schemas: format="csv", metadataVersion="3.0.1" (regardless of connector/user)
        is_connector_schema = bool(origin_id and adapter_id)
        # Check if schema is flat from document structure (not from JPK, so use direct check)
        schema_is_flat = schema_document.get('O', {}).get('customSchemaIsFlat', False) if isinstance(schema_document, dict) else False
        
        # Use rule-based format determination
        schema_format, metadata_version = get_schema_format(schema_is_flat, is_connector_schema)
        
        schema_component = {
            "checksum": "1",
            "type": 900,
            "name": schema_name,  # Use the exact name that transformations are referencing
            "id": schema_id,
            "filename": schema_id,
            "isCustom": True,
            "format": schema_format,  # CRITICAL: Flat schemas and connector schemas use "csv", user schemas use "xml"
            "schemaTypeDocument": schema_document,
            # Required metadata fields for Integration Studio
            # CRITICAL: Flat schemas use 3.0.1, connector schemas use 2.0.0, user schemas use 3.0.1
            "metadataVersion": metadata_version,
            "validationState": 100,  # Must be integer 100, not string "VALID"
            "requiresDeploy": True,
            "encryptedAtRest": True,
            "chunks": 1,
            "partial": False,
            # Additional fields from baseline pattern
            "displayName": schema_name,  # Baseline has displayName matching name
            "hidden": False  # Baseline has hidden=False
        }
        
        # CRITICAL FIX: Only add origin field if it has valid values
        # Baseline pattern: User/canonical schemas (like "Salesforce Query Response Schema") have NO origin
        # Connector schemas have origin with valid adapterId, functionName, direction, id
        if origin_id and adapter_id and function_name and direction:
            schema_component["origin"] = {
                "id": origin_id,
                "adapterId": adapter_id,
                "direction": direction,
                "functionName": function_name
            }
        # If origin exists but has None values, don't add it (matches baseline pattern for user schemas)
        
        print(f"         ‚úÖ Created Type 900 embedded schema: {schema_name} (structure from {structure_source})")
        return schema_component

    def _extract_jtr_from_cache(self, jpk_path: str, origin_id: str, direction: str) -> Optional[str]:
        """
        Extract JTR content from JPK cache file and convert to JSON format.
        
        The JPK stores JTR (Jitterbit Type Representation) as GZIP-compressed XML
        in cache/ConnectorCallStructures/{origin_id}_{direction}.gz.
        The JSON format requires ZLIB compression + base64 encoding.
        
        Args:
            jpk_path: Path to the JPK ZIP file
            origin_id: The connector origin ID (UUID)
            direction: 'input' or 'output'
            
        Returns:
            Base64-encoded ZLIB-compressed JTR XML string, or None if not found/error
        """
        if not jpk_path or not origin_id or not direction:
            return None
        
        cache_filename = f"{origin_id}_{direction}.gz"
        
        try:
            with zipfile.ZipFile(jpk_path, 'r') as jpk:
                # Find cache file (project folder name varies)
                cache_path = None
                for file_info in jpk.filelist:
                    if file_info.filename.endswith(f"cache/ConnectorCallStructures/{cache_filename}"):
                        cache_path = file_info.filename
                        break
                
                if not cache_path:
                    # Log cache file not found
                    if self.trace_logger:
                        self.trace_logger.log_decision(
                            f"JTR cache file not found: {cache_filename}",
                            {"origin_id": origin_id, "direction": direction, "found": False},
                            VerbosityLevel.NORMAL
                        )
                    return None
                
                # Read raw gz bytes from zip
                raw_gz = jpk.read(cache_path)
                raw_size = len(raw_gz)
                
                # GZIP decompress to get JTR XML
                jtr_xml = gzip.decompress(raw_gz)
                decompressed_size = len(jtr_xml)
                
                # ZLIB compress (level 9 for maximum compression)
                zlib_compressed = zlib.compress(jtr_xml, level=9)
                zlib_size = len(zlib_compressed)
                
                # Base64 encode
                jtr_b64 = base64.b64encode(zlib_compressed).decode('utf-8')
                b64_len = len(jtr_b64)
                
                # Log successful extraction
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"JTR extracted: {cache_filename}",
                        {"origin_id": origin_id, "direction": direction, "found": True},
                        VerbosityLevel.NORMAL
                    )
                    self.trace_logger.log_reasoning(
                        f"JTR conversion: {raw_size}‚Üí{decompressed_size}‚Üí{zlib_size}‚Üí{b64_len} chars",
                        {"origin_id": origin_id, "direction": direction},
                        VerbosityLevel.DETAILED
                    )
                    self.trace_logger.log_source_data(
                        f"JTR content for {origin_id}_{direction}",
                        {"first_100_chars": jtr_b64[:100], "length": b64_len},
                        VerbosityLevel.DEBUG
                    )
                
                print(f"         üì¶ JTR extracted: {raw_size}‚Üí{decompressed_size}‚Üí{zlib_size}‚Üí{b64_len} chars")
                return jtr_b64
                
        except gzip.BadGzipFile as e:
            if self.trace_logger:
                self.trace_logger.log_decision(
                    f"JTR extraction failed: corrupted gzip - {cache_filename}",
                    {"origin_id": origin_id, "direction": direction, "error": str(e)},
                    VerbosityLevel.NORMAL
                )
            print(f"         ‚ö†Ô∏è JTR extraction failed (bad gzip): {cache_filename}")
            return None
        except Exception as e:
            if self.trace_logger:
                self.trace_logger.log_decision(
                    f"JTR extraction failed: {cache_filename}",
                    {"origin_id": origin_id, "direction": direction, "error": str(e)},
                    VerbosityLevel.NORMAL
                )
            print(f"         ‚ö†Ô∏è JTR extraction failed: {cache_filename} - {e}")
            return None

    def _parse_jtr_element(self, element: ET.Element) -> Dict[str, Any]:
        """
        Parse a single JTR XML element into schema structure.
        
        JTR XML format:
        <elem>
          <N>elementName</N>
          <NS>namespace</NS>
          <MN>minOccurs</MN>
          <MX>maxOccurs</MX>
          <T>type</T>
          <NIL>false</NIL>
          <ATR>true</ATR>  <!-- isAttribute -->
          <C>...</C>  <!-- children -->
        </elem>
        
        Returns schema structure:
        {
          "N": "elementName",
          "NS": "namespace",
          "MN": 1,
          "MX": 1,
          "T": "string",
          "NIL": false,
          "ATR": true,
          "C": [...]
        }
        """
        result = {}
        
        # Parse N (name) - required
        n_elem = element.find('N')
        if n_elem is not None and n_elem.text:
            result['N'] = n_elem.text
        
        # Parse NS (namespace) - optional
        ns_elem = element.find('NS')
        if ns_elem is not None and ns_elem.text:
            result['NS'] = ns_elem.text
        
        # Parse MN (minOccurs) - convert to int, default 0
        mn_elem = element.find('MN')
        if mn_elem is not None and mn_elem.text:
            try:
                result['MN'] = int(mn_elem.text)
            except ValueError:
                result['MN'] = 0
        
        # Parse MX (maxOccurs) - keep as "unbounded" string or convert to int
        mx_elem = element.find('MX')
        if mx_elem is not None and mx_elem.text:
            if mx_elem.text.lower() == 'unbounded':
                result['MX'] = 'unbounded'
            else:
                try:
                    result['MX'] = int(mx_elem.text)
                except ValueError:
                    result['MX'] = 1
        
        # Parse T (type) - optional
        t_elem = element.find('T')
        if t_elem is not None and t_elem.text:
            result['T'] = t_elem.text
        
        # Parse DT (dataType) - optional
        dt_elem = element.find('DT')
        if dt_elem is not None and dt_elem.text:
            result['DT'] = dt_elem.text
        
        # Parse NIL (nillable) - convert to bool, default False
        nil_elem = element.find('NIL')
        if nil_elem is not None and nil_elem.text:
            result['NIL'] = nil_elem.text.lower() == 'true'
        
        # Parse ATR (isAttribute) - convert to bool, default False
        atr_elem = element.find('ATR')
        if atr_elem is not None and atr_elem.text:
            result['ATR'] = atr_elem.text.lower() == 'true'
        
        # Parse DV (defaultValue) - optional
        dv_elem = element.find('DV')
        if dv_elem is not None and dv_elem.text:
            result['DV'] = dv_elem.text
        
        # Parse I (index) - optional int
        i_elem = element.find('I')
        if i_elem is not None and i_elem.text:
            try:
                result['I'] = int(i_elem.text)
            except ValueError:
                pass
        
        # Parse L (level) - optional int
        l_elem = element.find('L')
        if l_elem is not None and l_elem.text:
            try:
                result['L'] = int(l_elem.text)
            except ValueError:
                pass
        
        # Parse BG (begin) - optional int
        bg_elem = element.find('BG')
        if bg_elem is not None and bg_elem.text:
            try:
                result['BG'] = int(bg_elem.text)
            except ValueError:
                pass
        
        # Parse EN (end) - optional int
        en_elem = element.find('EN')
        if en_elem is not None and en_elem.text:
            try:
                result['EN'] = int(en_elem.text)
            except ValueError:
                pass
        
        # Parse child elements recursively
        children = []
        for child_elem in element.findall('C'):
            child_result = self._parse_jtr_element(child_elem)
            if child_result:
                children.append(child_result)
        
        if children:
            result['C'] = children
        
        return result

    def _parse_jtr_xml_to_schema(self, jtr_xml_bytes: bytes) -> Optional[Dict[str, Any]]:
        """
        Parse JTR XML content into schema root structure.
        
        Args:
            jtr_xml_bytes: Raw JTR XML content (UTF-8 encoded)
            
        Returns:
            Schema root structure: {"N": "...", "NS": "...", "C": [...], ...}
            or None if parsing fails
        """
        if not jtr_xml_bytes:
            return None
        
        try:
            xml_size = len(jtr_xml_bytes)
            
            # Parse XML
            root = ET.fromstring(jtr_xml_bytes)
            
            # Handle JTR root element
            # The root element might be <JTR> with Type/Version attributes
            if root.tag == 'JTR':
                # Parse JTR content as if it's a schema element
                schema = self._parse_jtr_element(root)
            else:
                # Direct schema element
                schema = self._parse_jtr_element(root)
            
            if schema:
                element_count = self._count_schema_elements(schema)
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"JTR XML parsed successfully",
                        {"xml_size": xml_size, "element_count": element_count},
                        VerbosityLevel.DETAILED
                    )
                print(f"         üìã JTR parsed: {xml_size} bytes ‚Üí {element_count} elements")
                return schema
            else:
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"JTR XML parsing returned empty schema",
                        {"xml_size": xml_size},
                        VerbosityLevel.NORMAL
                    )
                return None
                
        except ET.ParseError as e:
            if self.trace_logger:
                self.trace_logger.log_decision(
                    f"JTR XML parse error",
                    {"error": str(e)},
                    VerbosityLevel.NORMAL
                )
            print(f"         ‚ö†Ô∏è JTR XML parse error: {e}")
            return None
        except Exception as e:
            if self.trace_logger:
                self.trace_logger.log_decision(
                    f"JTR XML parsing failed",
                    {"error": str(e)},
                    VerbosityLevel.NORMAL
                )
            print(f"         ‚ö†Ô∏è JTR XML parsing failed: {e}")
            return None

    def _count_schema_elements(self, schema: Dict[str, Any]) -> int:
        """Count total elements in a schema structure recursively."""
        count = 1  # Current element
        for child in schema.get('C', []):
            count += self._count_schema_elements(child)
        return count

    def _read_jtr_cache_raw(self, jpk_path: str, activity_id: str, direction: str) -> Optional[bytes]:
        """
        Read JTR XML from cache file without base64/zlib conversion.
        Returns raw decompressed XML bytes.
        
        Args:
            jpk_path: Path to the JPK ZIP file
            activity_id: The activity ID (UUID)
            direction: 'input' or 'output'
            
        Returns:
            Raw JTR XML bytes, or None if not found/error
        """
        if not jpk_path or not activity_id or not direction:
            return None
        
        cache_filename = f"{activity_id}_{direction}.gz"
        
        try:
            with zipfile.ZipFile(jpk_path, 'r') as jpk:
                # Find cache file
                cache_path = None
                for file_info in jpk.filelist:
                    if file_info.filename.endswith(f"cache/ConnectorCallStructures/{cache_filename}"):
                        cache_path = file_info.filename
                        break
                
                if not cache_path:
                    return None
                
                # Read and decompress
                raw_gz = jpk.read(cache_path)
                jtr_xml = gzip.decompress(raw_gz)
                
                return jtr_xml
                
        except Exception as e:
            print(f"         ‚ö†Ô∏è JTR cache read failed: {cache_filename} - {e}")
            return None

    def _generate_activity_schema_o_field(self, activity: Dict[str, Any], direction: str, 
                                           root_element_name: str, root_namespace: str = None) -> Dict[str, Any]:
        """
        Generate the O (options) field for activity schema.
        
        Args:
            activity: The activity component dict
            direction: 'input' or 'output'
            root_element_name: Name of the root element from parsed schema
            root_namespace: Namespace of the root element (optional)
            
        Returns:
            O field dict with file paths, root names, content type
        """
        adapter_id = activity.get('adapterId', '')
        function_name = activity.get('functionName', '')
        activity_id = activity.get('id', '')
        object_name = activity.get('objectName')
        
        # If objectName is not set, try to extract it from the endpoint name
        # Example: "NetSuite Upsert Contact" -> "Contact"
        if not object_name:
            activity_name = activity.get('name', '')
            if activity_name:
                # For NetSuite: "NetSuite Upsert Contact" -> "Contact"
                # For Salesforce: "Salesforce Query Contact" -> "Contact"
                parts = activity_name.split()
                if len(parts) >= 2:
                    # Take the last part as the object name
                    object_name = parts[-1]
                else:
                    object_name = 'Object'
            else:
                object_name = 'Object'
        
        # Build XSD file path pattern
        # Format: jitterbit.{adapter}.{activity_id}.{function}_{object}.{direction}.xsd
        xsd_base = f"jitterbit.{adapter_id}.{activity_id}.{function_name}_{object_name}"
        
        # Build root name with namespace
        root_name = root_element_name or ''
        if root_namespace and root_name:
            root_name = f"{{{root_namespace}}}{root_name}"
        
        o_field = {
            "requestStructureFilePath": f"{xsd_base}.request.xsd",
            "requestRootName": root_name if direction == 'input' else "",
            "responseStructureFilePath": f"{xsd_base}.request.xsd",  # Uses request.xsd for both
            "responseRootName": root_name if direction == 'output' else "",
            "contentType": "xml"
        }
        
        return o_field

    def _populate_activity_schemas(self, components: List[Dict[str, Any]], jpk_path: str) -> None:
        """
        Populate input and output schema fields for connector activities.
        
        Strategy: Find Type 900 schemas that match the activity's ID + direction,
        then copy their schemaTypeDocument contents to the activity's input/output fields.
        
        Args:
            components: List of all components (will modify in place)
            jpk_path: Path to JPK file for reading cache files
        """
        if self.trace_logger:
            self.trace_logger.log_decision(
                "Starting activity schema population",
                {"component_count": len(components), "jpk_path": jpk_path},
                VerbosityLevel.NORMAL
            )
        
        # Build a map of Type 900 schemas by (origin_id, direction) for backwards compatibility
        schema_map = {}
        # Also build a map by (adapterId, functionName, direction) for matching by adapter properties
        schema_map_by_adapter = {}
        for comp in components:
            if comp.get('type') == 900:
                origin = comp.get('origin', {})
                origin_id = origin.get('id')
                direction = origin.get('direction')
                adapter_id = origin.get('adapterId')
                function_name = origin.get('functionName')
                
                schema_doc = comp.get('schemaTypeDocument', {})
                if not schema_doc:
                    continue
                
                # Map by (origin_id, direction) for backwards compatibility
                if origin_id and direction:
                    key = (origin_id, direction)
                    schema_map[key] = schema_doc
                
                # Map by (adapterId, functionName, direction) for matching by adapter properties
                # This allows matching when endpoint IDs don't match schema origin.id
                if adapter_id and function_name and direction:
                    adapter_key = (adapter_id.lower(), function_name.lower(), direction)
                    if adapter_key not in schema_map_by_adapter:
                        schema_map_by_adapter[adapter_key] = schema_doc
                        if self.trace_logger:
                            self.trace_logger.log_decision(
                                f"Registered schema for adapter-based lookup",
                                {"adapter_id": adapter_id, "function_name": function_name, "direction": direction,
                                 "origin_id": origin_id, "has_root": 'root' in schema_doc, "has_jtr": 'jtr' in schema_doc},
                                VerbosityLevel.DEBUG
                            )
        
        print(f"   üìä Found {len(schema_map)} Type 900 schemas (by origin_id) and {len(schema_map_by_adapter)} (by adapter) for activity mapping")
        
        activities_processed = 0
        activities_with_schemas = 0
        
        for component in components:
            # Only process Type 500 activities
            if component.get('type') != 500:
                continue
            
            # Check if this is a connector activity that might have schemas
            adapter_id = component.get('adapterId')
            if not adapter_id:
                continue
            
            activity_id = component.get('id')
            activity_name = component.get('name', 'Unknown')
            activities_processed += 1
            
            # Verify we have a valid component ID before processing
            if not activity_id:
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"Skipping activity without ID: {activity_name}",
                        {"adapter_id": adapter_id},
                        VerbosityLevel.DETAILED
                    )
                continue
            
            if self.trace_logger:
                self.trace_logger.log_decision(
                    f"Processing activity: {activity_name}",
                    {"activity_id": activity_id, "adapter_id": adapter_id},
                    VerbosityLevel.DETAILED
                )
            
            schemas_added = False
            function_name = component.get('functionName', '')
            
            # Try to populate INPUT field from Type 900 schema
            # First try by ID (for backwards compatibility)
            input_key = (activity_id, 'input')
            schema_doc = None
            if input_key in schema_map:
                schema_doc = schema_map[input_key]
            elif function_name:
                # Try matching by (adapterId, functionName, direction)
                adapter_input_key = (adapter_id.lower(), function_name.lower(), 'input')
                if adapter_input_key in schema_map_by_adapter:
                    schema_doc = schema_map_by_adapter[adapter_input_key]
                    if self.trace_logger:
                        self.trace_logger.log_decision(
                            f"Input schema matched by adapter properties: {activity_name}",
                            {"adapter_id": adapter_id, "function_name": function_name, "direction": "input"},
                            VerbosityLevel.DETAILED
                        )
            
            if schema_doc:
                root_elem = schema_doc.get('root', {})
                root_name = root_elem.get('N', '') if root_elem else ''
                root_ns = root_elem.get('NS', '') if root_elem else ''
                
                # Generate O field if not present (connector schemas filter out O from schemaTypeDocument)
                o_field = schema_doc.get('O')
                if not o_field and root_name:
                    o_field = self._generate_activity_schema_o_field(component, 'input', root_name, root_ns)
                
                component['input'] = {
                    'root': root_elem
                }
                if o_field:
                    component['input']['O'] = o_field
                if 'jtr' in schema_doc:
                    component['input']['jtr'] = schema_doc['jtr']
                schemas_added = True
                print(f"         ‚úÖ Input schema added to: {activity_name}")
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"Input schema copied from Type 900: {activity_name}",
                        {"has_O": 'O' in component['input'], "has_root": 'root' in component['input'], "has_jtr": 'jtr' in component.get('input', {})},
                        VerbosityLevel.DETAILED
                    )
            else:
                # Fallback: Try to extract JTR from cache and build schema
                input_jtr_xml = self._read_jtr_cache_raw(jpk_path, activity_id, 'input')
                if input_jtr_xml:
                    input_schema = self._parse_jtr_xml_to_schema(input_jtr_xml)
                    if input_schema:
                        root_name = input_schema.get('N', '')
                        root_ns = input_schema.get('NS', '')
                        input_o = self._generate_activity_schema_o_field(component, 'input', root_name, root_ns)
                        # Also get the jtr base64 version
                        jtr_b64 = self._extract_jtr_from_cache(jpk_path, activity_id, 'input')
                        component['input'] = {'O': input_o, 'root': input_schema}
                        if jtr_b64:
                            component['input']['jtr'] = jtr_b64
                        schemas_added = True
                        print(f"         ‚úÖ Input schema (from cache) added to: {activity_name}")
            
            # Try to populate OUTPUT field from Type 900 schema
            # First try by ID (for backwards compatibility)
            output_key = (activity_id, 'output')
            schema_doc = None
            if output_key in schema_map:
                schema_doc = schema_map[output_key]
            elif function_name:
                # Try matching by (adapterId, functionName, direction)
                adapter_output_key = (adapter_id.lower(), function_name.lower(), 'output')
                if adapter_output_key in schema_map_by_adapter:
                    schema_doc = schema_map_by_adapter[adapter_output_key]
                    if self.trace_logger:
                        self.trace_logger.log_decision(
                            f"Output schema matched by adapter properties: {activity_name}",
                            {"adapter_id": adapter_id, "function_name": function_name, "direction": "output"},
                            VerbosityLevel.DETAILED
                        )
            
            if schema_doc:
                root_elem = schema_doc.get('root', {})
                root_name = root_elem.get('N', '') if root_elem else ''
                root_ns = root_elem.get('NS', '') if root_elem else ''
                
                # Generate O field if not present (connector schemas filter out O from schemaTypeDocument)
                o_field = schema_doc.get('O')
                if not o_field:
                    # Always generate O field for connector schemas, even if root_name is empty
                    # This ensures the output field structure is complete
                    if root_name or root_elem:  # Only generate if we have some structure
                        o_field = self._generate_activity_schema_o_field(component, 'output', root_name or 'root', root_ns)
                
                # Create output field structure
                component['output'] = {
                    'root': root_elem if root_elem else {}
                }
                if o_field:
                    component['output']['O'] = o_field
                if 'jtr' in schema_doc:
                    component['output']['jtr'] = schema_doc['jtr']
                schemas_added = True
                print(f"         ‚úÖ Output schema added to: {activity_name} (root: {bool(root_elem)}, O: {bool(o_field)})")
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"Output schema copied from Type 900: {activity_name}",
                        {"has_O": 'O' in component['output'], "has_root": 'root' in component['output'], 
                         "has_jtr": 'jtr' in component.get('output', {}), "root_keys": list(root_elem.keys()) if root_elem else []},
                        VerbosityLevel.DETAILED
                    )
            else:
                # Fallback: Try to extract JTR from cache and build schema
                output_jtr_xml = self._read_jtr_cache_raw(jpk_path, activity_id, 'output')
                if output_jtr_xml:
                    output_schema = self._parse_jtr_xml_to_schema(output_jtr_xml)
                    if output_schema:
                        root_name = output_schema.get('N', '')
                        root_ns = output_schema.get('NS', '')
                        output_o = self._generate_activity_schema_o_field(component, 'output', root_name, root_ns)
                        # Also get the jtr base64 version
                        jtr_b64 = self._extract_jtr_from_cache(jpk_path, activity_id, 'output')
                        component['output'] = {'O': output_o, 'root': output_schema}
                        if jtr_b64:
                            component['output']['jtr'] = jtr_b64
                        schemas_added = True
                        print(f"         ‚úÖ Output schema (from cache) added to: {activity_name}")
            
            if schemas_added:
                activities_with_schemas += 1
        
        print(f"   üìä Activity schema population: {activities_with_schemas}/{activities_processed} activities have schemas")
        if self.trace_logger:
            self.trace_logger.log_decision(
                "Activity schema population complete",
                {"processed": activities_processed, "with_schemas": activities_with_schemas},
                VerbosityLevel.NORMAL
            )

    def _merge_components(self, baseline: Dict[str, Any], components: Dict[str, Any]) -> Dict[str, Any]:
        """
        Merge extracted components with baseline.

        Args:
            baseline: Baseline JSON structure
            components: Extracted components

        Returns:
            Merged result dictionary
        """
        print("üîß Merging components with baseline...")

        existing_components = baseline.get('project', {}).get('components', [])

        # Remove baseline transformation components and use extracted ones
        non_transformation_components = [c for c in existing_components if c.get('type') != 700]
        extracted_transform_count = len(components['transformations'])
        print(f"   Replacing baseline stubs with {extracted_transform_count} extracted transformations")
        
        # Remove baseline operation components and use extracted ones
        non_operation_components = [c for c in non_transformation_components if c.get('type') != 200]
        
        # Remove baseline script components and use extracted ones
        non_script_components = [c for c in non_operation_components if c.get('type') != 400]
        extracted_script_count = len(components.get('type_400_scripts', []))
        if extracted_script_count > 0:
            print(f"   Replacing baseline scripts with {extracted_script_count} extracted Type 400 scripts from JPK")
        
        extracted_operation_count = len(components.get('operations', []))
        if extracted_operation_count > 0:
            print(f"   Replacing baseline operations with {extracted_operation_count} extracted operations")

        # Separate existing components by type for ordering
        existing_by_type = {}
        for comp in non_script_components:
            comp_type = comp.get('type', 'Unknown')
            if comp_type not in existing_by_type:
                existing_by_type[comp_type] = []
            existing_by_type[comp_type].append(comp)

        # Build ordered component list
        ordered_components = []

        # Build set of existing Type 600 endpoint names (case-insensitive) for deduplication
        existing_type_600_names = {
            c.get('name', '').lower()
            for c in existing_by_type.get(600, [])
        }

        # Add components in the correct order
        for comp_type in COMPONENT_ORDER:
            # Add existing components of this type
            if comp_type in existing_by_type:
                ordered_components.extend(existing_by_type[comp_type])

            # Add new components of this type
            if comp_type == 500:  # Type 500 endpoints
                ordered_components.extend(components['type_500_endpoints'])
            elif comp_type == 600:  # Type 600 endpoints
                # Deduplicate: only add extracted Type 600s that don't already exist in baseline
                for ep in components['type_600_endpoints']:
                    ep_name_lower = ep.get('name', '').lower()
                    if ep_name_lower not in existing_type_600_names:
                        ordered_components.append(ep)
                    else:
                        print(f"   ‚è≠Ô∏è  Skipping duplicate Type 600: {ep.get('name')} (already in baseline)")
            elif comp_type == 200:  # Operations
                # Add extracted operations
                ordered_components.extend(components.get('operations', []))
            elif comp_type == 700:  # Transformations
                # Add extracted transformations
                ordered_components.extend(components['transformations'])
            elif comp_type == 400:  # Scripts
                # Add extracted Type 400 scripts from JPK
                ordered_components.extend(components.get('type_400_scripts', []))
            elif comp_type == 900:  # Schema Document Components
                ordered_components.extend(components['schema_components'])
            elif comp_type == 1000:  # Project variables
                ordered_components.extend(components['project_variables'])
            elif comp_type == 1300:  # Global variables
                ordered_components.extend(components['global_variables'])

        # Add any remaining existing components that weren't in the standard order
        handled_types = set(COMPONENT_ORDER)
        for comp_type, comps in existing_by_type.items():
            if comp_type not in handled_types:
                ordered_components.extend(comps)

        # Update baseline with new components
        baseline['project']['components'] = ordered_components

        # CRITICAL FIX: Update workflows to use converted operation IDs
        # Workflows reference operations by ID, but converted operations have different IDs
        # Strategy: For the main workflow, include ALL converted operations to ensure completeness
        # Map baseline operation names to converted operation IDs for name-based matching
        baseline_ops_by_id = {
            c.get('id'): c.get('name') 
            for c in existing_components 
            if c.get('type') == 200
        }
        converted_ops_by_name = {
            c.get('name'): c.get('id') 
            for c in components.get('operations', [])
        }
        
        # Update workflows to use converted operation IDs
        workflows = baseline.get('project', {}).get('workflows', [])
        for workflow in workflows:
            workflow_name = workflow.get('name', '').lower()
            
            # For the main sync workflow, include ALL converted operations
            # This ensures "Test Email" and all other operations are included
            if 'sync-salesforce' in workflow_name or 'sync' in workflow_name:
                # Build list of all converted operations
                all_converted_ops = []
                for op in components.get('operations', []):
                    op_id = op.get('id')
                    op_name = op.get('name')
                    if op_id and op_name:
                        all_converted_ops.append({
                            'id': op_id,
                            'type': 200
                        })
                
                workflow['operations'] = all_converted_ops
                print(f"   ‚úÖ Updated workflow '{workflow.get('name')}' with {len(all_converted_ops)} operations (all converted operations included)")
            else:
                # For other workflows, try to map by name
                if 'operations' in workflow:
                    updated_operations = []
                    for op_ref in workflow['operations']:
                        # op_ref can be a dict with 'id' or just an ID string
                        if isinstance(op_ref, dict):
                            op_id = op_ref.get('id')
                        else:
                            op_id = op_ref
                        
                        # Find the operation name from baseline using the ID
                        op_name = baseline_ops_by_id.get(op_id)
                        
                        # Map to converted operation ID by name
                        if op_name and op_name in converted_ops_by_name:
                            new_op_id = converted_ops_by_name[op_name]
                            if isinstance(op_ref, dict):
                                # Preserve any additional properties (like writeActivity)
                                updated_op = op_ref.copy()
                                updated_op['id'] = new_op_id
                                updated_operations.append(updated_op)
                            else:
                                updated_operations.append(new_op_id)
                        else:
                            # Skip operations that couldn't be mapped (invalid IDs cause deployment failures)
                            if op_name:
                                print(f"   ‚ö†Ô∏è  Warning: Operation '{op_name}' not found in converted operations - skipping")
                            else:
                                print(f"   ‚ö†Ô∏è  Warning: Operation with ID '{op_id}' not found - skipping")

                    workflow['operations'] = updated_operations
                    if updated_operations:
                        print(f"   ‚úÖ Updated workflow '{workflow.get('name')}' with {len(updated_operations)} operations")
                    else:
                        print(f"   ‚ö†Ô∏è  Workflow '{workflow.get('name')}' has no valid operations")

        # Remove empty workflows (Jitterbit doesn't allow empty workflows)
        original_workflow_count = len(workflows)
        workflows[:] = [wf for wf in workflows if wf.get('operations')]
        removed_count = original_workflow_count - len(workflows)
        if removed_count > 0:
            print(f"   üßπ Removed {removed_count} empty workflow(s)")

        # Add XSD assets to the output (v321 feature)
        if components['xsd_assets']:
            baseline['assets'] = components['xsd_assets']

        # Update project name to be unique
        unique_name = f"JPK-{TARGET_VERSION}-Modular-{str(uuid.uuid4())[:8]}"
        baseline['project']['name'] = unique_name

        # Log summary
        print(f"   Added {len(components['project_variables'])} Project Variables (Type 1000)")
        print(f"   Added {len(components['global_variables'])} Global Variables (Type 1300)")
        print(f"   Added {len(components['type_500_endpoints'])} Type 500 Endpoints")
        print(f"   Added {len(components['type_600_endpoints'])} Type 600 Endpoints")
        print(f"   Added {len(components.get('type_400_scripts', []))} Scripts (Type 400)")
        print(f"   Added {len(components.get('operations', []))} Operations (Type 200)")
        print(f"   Added {len(components['transformations'])} Transformations (Type 700)")
        print(f"   Added {len(components['schema_components'])} Schema Document Components (Type 900)")
        print(f"   Added {len(components['xsd_assets'])} XSD Assets")
        print(f"   üìä Total components: {len(ordered_components)}")

        return baseline

    def _extract_transformations(self, jpk_path: str) -> List[Dict[str, Any]]:
        """
        Extract transformations from JPK using discovery tool and converter.
        
        Args:
            jpk_path: Path to JPK file
            
        Returns:
            List of transformation components
        """
        import subprocess
        import tempfile
        import os
        
        try:
            # Create temporary file for JPK discovery output
            temp_fd, temp_path = tempfile.mkstemp(suffix='.json')
            os.close(temp_fd)
            
            # Run JPK discovery tool
            discovery_script = Path(__file__).parent.parent.parent / 'jpk_discover_transformations.py'
            
            result = subprocess.run(
                ['python', str(discovery_script), jpk_path, temp_path],
                capture_output=True,
                text=True,
                check=False
            )
            
            if result.returncode != 0:
                print(f"   ‚ö†Ô∏è  JPK discovery failed: {result.stderr}")
                return []
            
            # Load discovery data
            with open(temp_path, 'r') as f:
                discovery_data = json.load(f)
            
            # Clean up temp file
            os.unlink(temp_path)
            
            # Convert transformations using the converter
            transformations = self.transformation_converter.convert_transformations_from_jpk_discovery(
                discovery_data
            )
            
            print(f"   üìä Extracted {len(transformations)} transformations from JPK")
            for transform in transformations:
                mapping_count = len(transform.get('mappingRules', []))
                print(f"      - {transform['name']}: {mapping_count} mapping rules")
            
            return transformations
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Transformation extraction failed: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _convert_operations(
        self,
        jpk_operations: List[Dict[str, Any]],
        transformations: List[Dict[str, Any]],
        type_500_endpoints: List[Dict[str, Any]],
        type_400_scripts: List[Dict[str, Any]] = None,
        activity_id_to_content_id: Dict[str, str] = None
    ) -> List[Dict[str, Any]]:
        """
        Convert JPK operations to Type 200 JSON operations.
        
        Args:
            jpk_operations: List of JPK operation dictionaries
            transformations: List of converted Type 700 transformation components (for step ID mapping)
            type_500_endpoints: List of converted Type 500 endpoint components (for step ID mapping)
            type_400_scripts: List of Type 400 script components (extracted from JPK, use content_id as component ID)
            activity_id_to_content_id: Mapping from activity_id to content_id for script step mapping
            
        Returns:
            List of Type 200 operation components
        """
        operations = []
        
        # Build mapping from JPK transformation ID to new JSON transformation ID
        # Also build set of content_ids that exist as Type 700 components (for skipping Request transformations)
        jpk_id_to_json_id = {}
        existing_transformation_content_ids = set()  # Set of content_ids that exist as Type 700 components
        for trans in transformations:
            metadata = trans.get('_conversion_metadata', {})
            original_jpk_id = metadata.get('original_jpk_id')
            if original_jpk_id:
                jpk_id_to_json_id[original_jpk_id] = trans.get('id')
                existing_transformation_content_ids.add(original_jpk_id)
        
        for jpk_op in jpk_operations:
            try:
                operation = self.operation_factory.create_operation(
                    operation_id=jpk_op['id'],
                    operation_name=jpk_op['name'],
                    activities=jpk_op.get('activities', []),
                    properties=jpk_op.get('properties', {}),
                    failure_operation_id=jpk_op.get('failure_operation_id'),
                    existing_transformation_ids=existing_transformation_content_ids
                )
                
                # Build endpoint lookup maps for matching
                # Map by (adapterId, functionName) -> list of endpoints (for disambiguation)
                endpoint_by_adapter_func = {}
                for endpoint in type_500_endpoints:
                    adapter = endpoint.get('adapterId', '')
                    func = endpoint.get('functionName', '')
                    key = (adapter, func)
                    if key not in endpoint_by_adapter_func:
                        endpoint_by_adapter_func[key] = []
                    endpoint_by_adapter_func[key].append(endpoint)
                
                # Map by ID for direct lookups
                endpoint_by_id = {e.get('id'): e for e in type_500_endpoints}
                
                # Build mapping from script component ID (content_id) to script component
                # Scripts now use content_id as component ID for RunScript() compatibility
                script_by_id = {s.get('id'): s for s in (type_400_scripts or [])}
                
                # Update step IDs to match converted component IDs
                # For Type 700 steps (transformations), map JPK content_id to new transformation ID
                # For Type 500 steps (endpoints), match by adapter+function or keep original ID
                updated_steps = []
                
                # Get the original activity data for this operation to help with matching
                # Create a map: step index -> activity (for endpoint/script steps only)
                jpk_activities = jpk_op.get('activities', [])
                activity_to_step_map = {}  # Maps activity index to step index
                step_to_activity_map = {}  # Maps step index to activity index
                
                # Build mapping: for each endpoint/script step, find corresponding activity
                # by matching the step_id (which is activity_id) with activity's activity_id
                for step_idx, step in enumerate(operation.get('steps', [])):
                    step_id = step.get('id')
                    step_type = step.get('type')
                    
                    # Only map endpoint and script steps to activities
                    if step_type in [400, 500]:
                        for act_idx, activity in enumerate(jpk_activities):
                            if activity.get('activity_id') == step_id:
                                step_to_activity_map[step_idx] = act_idx
                                break
                
                for step_idx, step in enumerate(operation.get('steps', [])):
                    step_id = step.get('id')
                    step_type = step.get('type')
                    
                    # For transformations (Type 700), map JPK content_id to new transformation ID
                    if step_type == 700 and step_id:
                        # step_id is the JPK content_id (original transformation ID from JPK)
                        # Map it to the new JSON transformation ID
                        mapped_id = jpk_id_to_json_id.get(step_id)
                        if mapped_id:
                            step['id'] = mapped_id
                        else:
                            print(f"   ‚ö†Ô∏è  Warning: Could not map transformation step ID {step_id[:8]}... to new transformation ID")
                    
                    # For endpoints (Type 500), match by adapter+function or direct ID
                    elif step_type == 500 and step_id:
                        # Try direct ID match first
                        matching_endpoint = endpoint_by_id.get(step_id)
                        
                        if not matching_endpoint:
                            # Try to match by adapter + function based on activity role
                            activity_idx = step_to_activity_map.get(step_idx)
                            if activity_idx is not None and activity_idx < len(jpk_activities):
                                activity = jpk_activities[activity_idx]
                                role = activity.get('role', '').lower()
                                activity_type = str(activity.get('type', ''))
                                
                                # NetSuite Function (type 232) -> netsuite + upsert
                                if ('netsuite' in role and 'function' in role) or activity_type == '232':
                                    candidates = endpoint_by_adapter_func.get(('netsuite', 'upsert'), [])
                                    # Prefer endpoint without '_old' suffix
                                    matching_endpoint = next((e for e in candidates if '_old' not in e.get('name', '')), None)
                                    if not matching_endpoint and candidates:
                                        matching_endpoint = candidates[0]
                                
                                # Source/Target activities - check multiple possibilities
                                elif role in ['source', 'target']:
                                    # Source activities can be:
                                    # 1. Salesforce Query (for reading Salesforce data)
                                    # 2. TempStorage read (for reading temp storage)
                                    # 
                                    # IMPORTANT: For NetSuite operations, prioritize TempStorage to avoid
                                    # multiple SOAP activities (Jitterbit rule: only one SOAP activity per operation)
                                    if role == 'source':
                                        # Check if this is a NetSuite operation (has NetSuite Function activity)
                                        # Role can be "NetSuite Function" (with capital letters and space)
                                        is_netsuite_operation = any(
                                            'netsuite' in a.get('role', '').lower() and 'function' in a.get('role', '').lower() or 
                                            str(a.get('type', '')) == '232'
                                            for a in jpk_activities
                                        )
                                        
                                        if is_netsuite_operation:
                                            # For NetSuite operations, prioritize TempStorage to avoid multiple SOAP activities
                                            candidates = endpoint_by_adapter_func.get(('tempstorage', 'tempstorage_read'), [])
                                            if candidates:
                                                # If multiple candidates, prefer one with "Canonical" or "Contact" in name
                                                # (matches the Source activity which reads canonical contact data)
                                                if len(candidates) > 1:
                                                    canonical_candidates = [e for e in candidates if 'canonical' in e.get('name', '').lower() or 'contact' in e.get('name', '').lower()]
                                                    if canonical_candidates:
                                                        matching_endpoint = canonical_candidates[0]
                                                    else:
                                                        matching_endpoint = candidates[0]
                                                else:
                                                    matching_endpoint = candidates[0]
                                            
                                            # Fallback to Salesforce Query if TempStorage not found
                                            if not matching_endpoint:
                                                candidates = endpoint_by_adapter_func.get(('salesforce', 'query'), [])
                                                if candidates:
                                                    matching_endpoint = candidates[0] if len(candidates) == 1 else None
                                        else:
                                            # For non-NetSuite operations, try Salesforce Query first (common pattern)
                                            candidates = endpoint_by_adapter_func.get(('salesforce', 'query'), [])
                                            if candidates:
                                                matching_endpoint = candidates[0] if len(candidates) == 1 else None
                                            
                                            # Fallback to tempstorage if Salesforce not found
                                            if not matching_endpoint:
                                                candidates = endpoint_by_adapter_func.get(('tempstorage', 'tempstorage_read'), [])
                                                if candidates:
                                                    matching_endpoint = candidates[0] if len(candidates) == 1 else None
                                    else:
                                        # Target activities are typically tempstorage write
                                        candidates = endpoint_by_adapter_func.get(('tempstorage', 'tempstorage_write'), [])
                                        if candidates:
                                            # If multiple candidates, try to match by name hint from operation context
                                            # Otherwise use first candidate as fallback
                                            if len(candidates) == 1:
                                                matching_endpoint = candidates[0]
                                            else:
                                                # For now, use first candidate as fallback
                                                # TODO: Improve matching logic to identify specific endpoint based on operation context
                                                matching_endpoint = candidates[0]
                                
                                # Salesforce Query -> salesforce + query (explicit Salesforce role)
                                # Also handle "Web Service Call" role which can be Salesforce Query
                                elif 'salesforce' in role or activity_type == '14' or role.lower() == 'web service call':
                                    candidates = endpoint_by_adapter_func.get(('salesforce', 'query'), [])
                                    if candidates:
                                        matching_endpoint = candidates[0] if len(candidates) == 1 else None
                            
                            if matching_endpoint:
                                step['id'] = matching_endpoint['id']
                            else:
                                activity_role = activity.get('role', 'N/A') if activity_idx is not None and activity_idx < len(jpk_activities) else 'N/A'
                                print(f"   ‚ö†Ô∏è  Warning: Could not find matching endpoint for step ID {step_id[:8]}... (role: {activity_role})")
                    
                    # For scripts (Type 400), map JPK activity_id to Type 400 script component ID
                    # Scripts now use content_id as component ID (for RunScript() compatibility)
                    # Operation steps use activity_id, so we need to map activity_id ‚Üí content_id
                    elif step_type == 400 and step_id:
                        # step_id is activity_id, need to map to content_id
                        content_id = (activity_id_to_content_id or {}).get(step_id)
                        
                        if content_id:
                            # Find script by content_id
                            matching_script = script_by_id.get(content_id)
                            if matching_script:
                                # Update step ID to use content_id (script component ID)
                                step['id'] = content_id
                            else:
                                print(f"   ‚ö†Ô∏è  Warning: Script with content_id {content_id[:8]}... not found (mapped from activity_id {step_id[:8]}...)")
                        else:
                            # Try direct match (in case step_id is already content_id)
                            matching_script = script_by_id.get(step_id)
                            if not matching_script:
                                activity_idx = step_to_activity_map.get(step_idx)
                                if activity_idx is not None and activity_idx < len(jpk_activities):
                                    activity = jpk_activities[activity_idx]
                                    print(f"   ‚ö†Ô∏è  Warning: Script step ID {step_id[:8]}... (activity_id) could not be mapped to content_id. Script may not have been extracted from JPK.")
                            # If direct match works, step_id is already correct
                            pass
                    
                    updated_steps.append(step)
                
                operation['steps'] = updated_steps
                operations.append(operation)
                
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error converting operation {jpk_op.get('name', 'Unknown')}: {e}")
                continue
        
        return operations

    def _save_result(self, result: Dict[str, Any], output_path: str) -> None:
        """
        Save the conversion result to JSON file.

        Args:
            result: Final conversion result
            output_path: Path to save the result
        """
        print(f"üíæ Saving converted JSON to {output_path}...")

        try:
            # Add converter version metadata
            result['_converter'] = get_version_info()

            with open(output_path, 'w') as f:
                json.dump(result, f, separators=(',', ':'))

            print(f"‚úÖ Conversion complete!")
            print(f"   üìÅ Output: {output_path}")
            print(f"   üìù Project name: {result['project']['name']}")
            print(f"   üîß {get_version_string()}")

        except Exception as e:
            raise ConfigurationError(f"Error saving output file {output_path}: {e}")
    
    def _link_transformation_schema_ids(self, transformations: List[Dict[str, Any]], schema_components: List[Dict[str, Any]]) -> None:
        """
        Link transformation source/target IDs to Type 900 component IDs.
        
        CRITICAL FIX (December 12, 2025):
        - Connector schemas (with origin): MUST have source.id/target.id matching Type 900 component ID
        - This enables ID-based lookup in getTransformationSchemaDetail (main-EBGGZ3NW.js:119634)
        - Without source.id, fallback findDocumentByOrigin fails, causing component-level validation errors
        - User schemas (with document, no origin): YES target.id in transformation
        
        This method:
        1. Builds lookup maps: schema_name -> Type 900 component ID, and origin.id+direction -> Type 900 component ID
        2. Adds source.id for connector schemas (matching Type 900 component by origin.id + direction)
        3. Adds target.id for user schemas (matching Type 900 component by name)
        
        Args:
            transformations: List of Type 700 transformation components
            schema_components: List of Type 900 schema components
        """
        # Build lookup map: schema_name -> Type 900 component ID (for user schemas)
        schema_name_to_id = {}
        for schema_comp in schema_components:
            schema_name = schema_comp.get('name')
            schema_id = schema_comp.get('id')
            if schema_name and schema_id:
                schema_name_to_id[schema_name] = schema_id
        
        # Build lookup map: (origin.id, direction) -> Type 900 component ID (for connector schemas)
        # Also build: schema_name -> Type 900 component ID (for exact name matching)
        origin_to_id = {}
        schema_name_to_id_connector = {}  # For connector schemas by name
        for schema_comp in schema_components:
            origin = schema_comp.get('origin')
            schema_name = schema_comp.get('name')
            schema_id = schema_comp.get('id')
            if origin and schema_id:
                origin_id = origin.get('id')
                direction = origin.get('direction')
                if origin_id and direction:
                    key = (origin_id, direction)
                    # If multiple schemas share same origin_id+direction, prefer exact name match
                    # Store all matches, but prioritize by name if available
                    if key not in origin_to_id:
                        origin_to_id[key] = schema_id
                    # Also store by name for exact matching
                    if schema_name:
                        schema_name_to_id_connector[schema_name] = schema_id
        
        # Update transformations to include source.id for connector schemas and target.id for user schemas
        updated_count = 0
        for transform in transformations:
            # CRITICAL FIX: Add source.id for connector schemas (matching Type 900 component ID)
            # This fixes component-level validation errors (investigation_summary_2025-12-11.md)
            # Use rule-based Response transformation detection
            transform_name = transform.get('name', '')
            mapping_rules = transform.get('mappingRules', [])
            is_response_with_script = should_remove_source_origin(transform_name, mapping_rules)
            
            source = transform.get('source')
            if source:
                source_origin = source.get('origin')
                if source_origin:
                    # CRITICAL FIX (December 15, 2025): Keep origin.id for connector schemas
                    # The validation expects source.origin.id === activity.id (main-EBGGZ3NW.js:115745)
                    # Baseline Response transformation has origin.id pointing to activity ID, not a Type 900 schema ID
                    # Removing origin causes "source schema does not match" validation error
                    # For ALL connector schemas (including Response transformations), keep origin.id pointing to activity
                    if False:  # Disabled: was removing origin for Response transformations, but baseline shows origin is required
                        # Find the Type 900 schema by name
                        source_name = source.get('name')
                        if source_name:
                            source_schema = next((s for s in schema_components if s.get('name') == source_name), None)
                            if source_schema:
                                source['id'] = source_schema.get('id')
                                # Remove origin for Response transformations (working reference pattern)
                                del source['origin']
                                updated_count += 1
                                if self.trace_logger:
                                    self.trace_logger.log_decision(
                                        f"Updated Response transformation source: {transform_name}",
                                        {"source_name": source_name, "source_id": source_schema.get('id'),
                                         "reason": "Working reference pattern: source.id set, origin removed (investigation_summary_2025-12-13.md)"},
                                        VerbosityLevel.DETAILED
                                    )
                    else:
                        # Connector schema - find matching Type 900 component
                        # CRITICAL FIX (December 14, 2025): When multiple schemas share same origin,
                        # prefer the one with complete nested structure needed by transformation's srcPaths
                        # This fixes issue: "NetSuite Upsert Contact - Response" transformation source
                        # pointing to connector function output instead of Type 900 schema with writeResponse elements
                        source_name = source.get('name')
                        origin_id = source_origin.get('id')
                        direction = source_origin.get('direction')
                        
                        schema_id = None
                        match_method = None
                        
                        # Get transformation's srcPaths to determine required structure
                        src_paths = []
                        for mr in transform.get('mappingRules', []):
                            mr_src_paths = mr.get('srcPaths', [])
                            if mr_src_paths:
                                src_paths.extend(mr_src_paths)
                        
                        # Priority 1: Match by exact name (most reliable when multiple schemas share origin_id+direction)
                        if source_name and source_name in schema_name_to_id_connector:
                            candidate_id = schema_name_to_id_connector[source_name]
                            # Verify this schema has the required structure if srcPaths are specified
                            if src_paths and self._schema_has_required_structure(schema_components, candidate_id, src_paths):
                                schema_id = candidate_id
                                match_method = "name_with_structure"
                            elif not src_paths:
                                # No srcPaths to check, use name match
                                schema_id = candidate_id
                                match_method = "name"
                        
                        # Priority 2: If no name match or structure mismatch, find best match by origin.id + direction
                        if not schema_id and origin_id and direction:
                            key = (origin_id, direction)
                            # Find all schemas with this origin
                            candidate_schemas = [
                                (sc.get('id'), sc) for sc in schema_components
                                if sc.get('origin', {}).get('id') == origin_id and 
                                   sc.get('origin', {}).get('direction') == direction
                            ]
                            
                            if candidate_schemas:
                                if src_paths:
                                    # Prefer schema with complete structure
                                    for cand_id, cand_schema in candidate_schemas:
                                        if self._schema_has_required_structure(schema_components, cand_id, src_paths):
                                            schema_id = cand_id
                                            match_method = "origin_id+direction_with_structure"
                                            break
                                
                                # Fallback to first match if no structure match found
                                if not schema_id:
                                    schema_id = candidate_schemas[0][0]
                                    match_method = "origin_id+direction"
                        
                        if schema_id:
                            if not source.get('id') or source.get('id') != schema_id:
                                source['id'] = schema_id
                                updated_count += 1
                                if self.trace_logger:
                                    self.trace_logger.log_decision(
                                        f"Added source.id to connector schema: {transform_name}",
                                        {"source_name": source_name, "source_id": schema_id, 
                                         "match_method": match_method,
                                         "origin_id": origin_id, "direction": direction,
                                         "src_paths_count": len(src_paths),
                                         "reason": "Fixes component-level validation and source schema selection (investigation_summary_2025-12-13.md)"},
                                        VerbosityLevel.DETAILED
                                    )
            
            # Update target.id and target.name for user schemas (without origin)
            target = transform.get('target')
            if target:
                # Only add target.id if it's a user schema (has document, no origin)
                has_document = bool(target.get('document'))
                has_origin = bool(target.get('origin'))
                
                if has_document and not has_origin:
                    target_name = target.get('name')
                    # CRITICAL FIX: For flat schemas, check if there's a Type 900 schema with matching structure
                    # and update both name and ID to match (fixes "Target Schema" vs "New Flat Schema" mismatch)
                    target_doc = target.get('document', {})
                    is_flat = target_doc.get('O', {}).get('customSchemaIsFlat', False) if isinstance(target_doc, dict) else False
                    
                    if is_flat:
                        # Find flat schema by structure match (customSchemaIsFlat=True)
                        for schema_comp in schema_components:
                            schema_doc = schema_comp.get('schemaTypeDocument', {})
                            schema_is_flat = schema_doc.get('O', {}).get('customSchemaIsFlat', False) if isinstance(schema_doc, dict) else False
                            if schema_is_flat:
                                # Found matching flat schema - update target name and ID
                                schema_name = schema_comp.get('name')
                                schema_id = schema_comp.get('id')
                                if schema_name and schema_id:
                                    # Update target name to match Type 900 schema name
                                    if target.get('name') != schema_name:
                                        target['name'] = schema_name
                                        updated_count += 1
                                    # Update target ID
                                    if not target.get('id') or target.get('id') != schema_id:
                                        target['id'] = schema_id
                                        updated_count += 1
                                    # CRITICAL: Update target.document.name to match schema name
                                    # This is required for validation (reference pattern)
                                    target_doc = target.get('document', {})
                                    if isinstance(target_doc, dict) and target_doc.get('name') != schema_name:
                                        target_doc['name'] = schema_name
                                        updated_count += 1
                                    if self.trace_logger:
                                        self.trace_logger.log_decision(
                                            f"Updated flat schema target name/ID/document.name: {transform.get('name')}",
                                            {"old_name": target_name, "new_name": schema_name, "target_id": schema_id},
                                            VerbosityLevel.DETAILED
                                        )
                                    break
                    elif target_name and target_name in schema_name_to_id:
                        # Non-flat user schema - match by name
                        target_id = schema_name_to_id[target_name]
                        if not target.get('id') or target.get('id') != target_id:
                            target['id'] = target_id
                            updated_count += 1
                            if self.trace_logger:
                                self.trace_logger.log_decision(
                                    f"Added target.id to user schema: {transform.get('name')}",
                                    {"target_name": target_name, "target_id": target_id},
                                    VerbosityLevel.DETAILED
                                )
        
        if updated_count > 0:
            print(f"   ‚úÖ Linked {updated_count} transformation schema IDs to Type 900 components")
    
    def _remove_source_from_first_step_transformations(
        self, 
        transformations: List[Dict[str, Any]], 
        operations: List[Dict[str, Any]]
    ) -> None:
        """
        Remove source schema from transformations that are the first step in operations.
        
        CRITICAL FIX (December 15, 2025):
        - Validation requires: if transformation has source schema, it must not be first step
        - Error: "OperationSourceActivityIsRequired" (main-EBGGZ3NW.js:115864-115878)
        - If transformation is first step AND has source schema ‚Üí remove source schema
        - This fixes Query Contacts operation validation error
        
        Args:
            transformations: List of Type 700 transformation components
            operations: List of Type 200 operation components
        """
        updated_count = 0
        
        # Build map: transformation_id -> is_first_step
        # Also check by original_jpk_id in case step IDs haven't been mapped yet
        transform_id_to_is_first = {}
        transform_jpk_id_to_is_first = {}
        
        for operation in operations:
            steps = operation.get('steps', [])
            if steps:
                first_step = steps[0]
                first_step_id = first_step.get('id')
                first_step_type = first_step.get('type')
                # Only mark transformations (type 700) as first step
                if first_step_type == 700 and first_step_id:
                    transform_id_to_is_first[first_step_id] = True
                    # Also check if this might be a JPK content_id that needs mapping
                    transform_jpk_id_to_is_first[first_step_id] = True
        
        # Remove source schema from transformations that are first step
        for transform in transformations:
            transform_id = transform.get('id')
            metadata = transform.get('_conversion_metadata', {})
            original_jpk_id = metadata.get('original_jpk_id')
            
            # Check both current ID and original JPK ID
            is_first_step = (transform_id in transform_id_to_is_first or 
                           (original_jpk_id and original_jpk_id in transform_jpk_id_to_is_first))
            
            if is_first_step:
                source = transform.get('source')
                if source:
                    # Check if source has origin (connector schema) - these are the problematic ones
                    if source.get('origin'):
                        # Remove source schema to fix validation error
                        del transform['source']
                        updated_count += 1
                        transform_name = transform.get('name', 'Unknown')
                        print(f"   üîß Removed source schema from first-step transformation: {transform_name}")
                        if self.trace_logger:
                            self.trace_logger.log_decision(
                                f"Removed source schema from first-step transformation: {transform_name}",
                                {"transform_id": transform_id, "original_jpk_id": original_jpk_id,
                                 "reason": "Fixes OperationSourceActivityIsRequired validation error (main-EBGGZ3NW.js:115864-115878)"},
                                VerbosityLevel.NORMAL
                            )
        
        if updated_count > 0:
            print(f"   üìä Removed source schema from {updated_count} first-step transformation(s)")
    
    def _update_transformation_origin_ids(
        self,
        transformations: List[Dict[str, Any]],
        operations: List[Dict[str, Any]]
    ) -> None:
        """
        Update source.origin.id in transformations to point to correct activity IDs based on operation step order.

        CRITICAL FIX (December 17, 2025):
        - After operations are converted, JPK activity IDs in source.origin.id need to be mapped to new endpoint/activity IDs
        - Validation expects source.origin.id === activity.id for the activity that appears before the transformation in operation steps
        - This fixes "Query Contacts Response" transformation validation where origin.id doesn't match the first step activity

        Args:
            transformations: List of Type 700 transformation components
            operations: List of Type 200 operation components
        """
        # Build map: transformation_id -> (operation, step_index)
        transform_to_operation = {}
        for operation in operations:
            steps = operation.get('steps', [])
            for step_idx, step in enumerate(steps):
                if step.get('type') == 700:  # Transformation step
                    transform_id = step.get('id')
                    if transform_id:
                        transform_to_operation[transform_id] = (operation, step_idx)
        
        updated_count = 0
        for transform in transformations:
            transform_id = transform.get('id')
            source = transform.get('source')
            
            if not source or not transform_id:
                continue
            
            origin = source.get('origin')
            if not origin:
                continue

            # CRITICAL: Skip Response transformations for Salesforce Update/Insert operations
            # Response transformations have their source from the SOAP response (same WebServiceCall),
            # NOT from a preceding activity. Their origin.id should remain as the WebServiceCall ID.
            # We identify these by checking if the function is NOT 'query' (e.g., 'update', 'insert')
            function_name = origin.get('functionName', '')
            adapter_id = origin.get('adapterId', '')
            if adapter_id == 'salesforce' and function_name in ['update', 'insert', 'delete', 'upsert']:
                # This is a Salesforce non-query Response transformation
                # The origin.id is already correctly set to the WebServiceCall ID
                # Skip updating it to avoid pointing to the wrong activity
                continue

            # Find which operation contains this transformation and its step index
            if transform_id not in transform_to_operation:
                continue

            operation, transform_step_idx = transform_to_operation[transform_id]
            steps = operation.get('steps', [])

            # Find the activity step that appears before this transformation
            # The origin.id should point to this activity
            source_activity_id = None
            for step_idx in range(transform_step_idx - 1, -1, -1):  # Look backwards from transformation step
                if step_idx < len(steps):
                    step = steps[step_idx]
                    step_type = step.get('type')
                    # Look for Type 500 (endpoint/activity) or Type 400 (script) steps
                    if step_type in [400, 500]:
                        source_activity_id = step.get('id')
                        break

            if source_activity_id and origin.get('id') != source_activity_id:
                old_origin_id = origin.get('id', '')
                origin['id'] = source_activity_id
                updated_count += 1
                transform_name = transform.get('name', 'Unknown')
                print(f"   üîß Updated origin.id for transformation: {transform_name} ({old_origin_id[:8]}... ‚Üí {source_activity_id[:8]}...)")
                if self.trace_logger:
                    self.trace_logger.log_decision(
                        f"Updated origin.id for transformation: {transform_name}",
                        {"transform_id": transform_id,
                         "old_origin_id": old_origin_id,
                         "new_origin_id": source_activity_id,
                         "operation": operation.get('name', 'Unknown'),
                         "reason": "Map JPK activity ID to converted endpoint/activity ID based on operation step order"},
                        VerbosityLevel.NORMAL
                    )
        
        if updated_count > 0:
            print(f"   üìä Updated origin.id for {updated_count} transformation(s)")
    
    def _schema_has_required_structure(self, schema_components: List[Dict[str, Any]], schema_id: str, src_paths: List[str]) -> bool:
        """
        Check if a Type 900 schema has the required nested structure for transformation srcPaths.
        
        This is used to select the correct Type 900 schema when multiple schemas exist with the same origin.
        For example, NetSuite response transformations need schemas with writeResponse elements,
        not just connector function outputs without the nested structure.
        
        Args:
            schema_components: List of all Type 900 schema components
            schema_id: ID of the schema to check
            src_paths: List of source paths required by the transformation (e.g., 
                      ["jbroot/jbresponse/upsertListResponse/writeResponseList/writeResponse/status/isSuccess"])
        
        Returns:
            True if schema contains all required paths, False otherwise
        """
        if not src_paths:
            return True  # No requirements, any schema works
        
        # Find the schema component
        schema_comp = next((sc for sc in schema_components if sc.get('id') == schema_id), None)
        if not schema_comp:
            return False
        
        # Get the schema document root
        schema_doc = schema_comp.get('schemaTypeDocument', {})
        root = schema_doc.get('root', {})
        if not root:
            return False
        
        # Check each required path
        for src_path in src_paths:
            if not self._path_exists_in_schema(root, src_path):
                return False
        
        return True
    
    def _path_exists_in_schema(self, root: Dict[str, Any], path: str) -> bool:
        """
        Check if a path exists in a schema document structure.
        
        Handles jbroot/jbresponse prefixes and array element notation (e.g., "1" in baseRef/1/RecordRef).
        
        Args:
            root: Schema document root node
            path: Path to check (e.g., "jbroot/jbresponse/upsertListResponse/writeResponseList/writeResponse/status/isSuccess")
        
        Returns:
            True if path exists, False otherwise
        """
        if not path or not root:
            return False
        
        # Split path into segments
        segments = path.split('/')
        
        # Remove jbroot and jbresponse prefixes if present (schema starts from upsertListResponse)
        if segments and segments[0] == 'jbroot':
            segments = segments[1:]
        if segments and segments[0] == 'jbresponse':
            segments = segments[1:]
        
        # Remove numeric array indices (e.g., "1" in baseRef/1/RecordRef) for schema matching
        # Also remove "RecordRef" if it's redundant (e.g., baseRef/RecordRef)
        cleaned_segments = []
        for i, seg in enumerate(segments):
            # Skip numeric segments that are array indices
            if seg.isdigit() and cleaned_segments and cleaned_segments[-1] in ['baseRef']:
                continue
            # Skip "RecordRef" if it's redundant after baseRef
            if seg == 'RecordRef' and cleaned_segments and cleaned_segments[-1] == 'baseRef':
                continue
            # Remove $ suffix if present
            seg = seg.rstrip('$')
            cleaned_segments.append(seg)
        
        # Traverse the schema structure
        current_node = root
        for seg in cleaned_segments:
            found = False
            # Check children
            if 'C' in current_node:
                for child in current_node['C']:
                    if child.get('N') == seg:
                        current_node = child
                        found = True
                        break
            if not found:
                return False
        
        return True
    
    def _validate_transformation_schema_references(self, transformations: List[Dict[str, Any]], schema_components: List[Dict[str, Any]]) -> None:
        """
        Validate that all transformation source/target schema references can be resolved to Type 900 components.
        
        This ensures transformations won't fail validation due to missing schemas.
        Fixes issue: "NetSuite Upsert Contact - Response" transformation missing target schema "New Flat Schema"
        
        Args:
            transformations: List of Type 700 transformation components
            schema_components: List of Type 900 schema components
        """
        # Build lookup maps
        schema_name_to_id = {}
        schema_id_set = set()
        for schema_comp in schema_components:
            schema_name = schema_comp.get('name')
            schema_id = schema_comp.get('id')
            if schema_name:
                schema_name_to_id[schema_name] = schema_id
            if schema_id:
                schema_id_set.add(schema_id)
        
        missing_schemas = []
        
        for transform in transformations:
            trans_name = transform.get('name', 'Unknown')
            
            # Check source schema
            source = transform.get('source', {})
            source_name = source.get('name')
            source_id = source.get('id')
            
            if source_name:
                # Check if schema exists by name
                if source_name not in schema_name_to_id:
                    missing_schemas.append({
                        'transformation': trans_name,
                        'role': 'source',
                        'schema_name': source_name,
                        'schema_id': source_id,
                        'issue': 'Schema name not found in Type 900 components'
                    })
                # If source has an ID, verify it exists
                elif source_id and source_id not in schema_id_set:
                    missing_schemas.append({
                        'transformation': trans_name,
                        'role': 'source',
                        'schema_name': source_name,
                        'schema_id': source_id,
                        'issue': 'Source ID does not match any Type 900 component ID'
                    })
            
            # Check target schema
            target = transform.get('target', {})
            target_name = target.get('name')
            target_id = target.get('id')
            
            if target_name:
                # Check if schema exists by name
                if target_name not in schema_name_to_id:
                    missing_schemas.append({
                        'transformation': trans_name,
                        'role': 'target',
                        'schema_name': target_name,
                        'schema_id': target_id,
                        'issue': 'Schema name not found in Type 900 components'
                    })
                # If target has an ID, verify it exists
                elif target_id and target_id not in schema_id_set:
                    missing_schemas.append({
                        'transformation': trans_name,
                        'role': 'target',
                        'schema_name': target_name,
                        'schema_id': target_id,
                        'issue': 'Target ID does not match any Type 900 component ID'
                    })
        
        # Report missing schemas
        if missing_schemas:
            print(f"   ‚ö†Ô∏è  WARNING: Found {len(missing_schemas)} transformation schema reference(s) that cannot be resolved:")
            for missing in missing_schemas:
                print(f"      - {missing['transformation']}: {missing['role']} schema '{missing['schema_name']}' - {missing['issue']}")
            if self.trace_logger:
                self.trace_logger.log_decision(
                    "Transformation schema validation found missing references",
                    {"missing_count": len(missing_schemas), "missing_schemas": missing_schemas},
                    VerbosityLevel.DETAILED
                )
        else:
            print(f"   ‚úÖ Validated {len(transformations)} transformation(s): All schema references resolved")
